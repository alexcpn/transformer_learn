{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMOlIbLXGN/n1eiRDAZipkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ec9420f6d8b456695eae721f86f0e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9f615e60b86409c8a63f79955e1b42b",
              "IPY_MODEL_158dc0bd2199425bb4e7158e3d38069d",
              "IPY_MODEL_1a1cfc559f2a4e20804581ce14eca755"
            ],
            "layout": "IPY_MODEL_da3b1176a0524f8694c548dedd84d20f"
          }
        },
        "f9f615e60b86409c8a63f79955e1b42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbcb68f6aec24c2a9a3e6317c9bf3fcc",
            "placeholder": "​",
            "style": "IPY_MODEL_9d5dd424dbc148038b6af91d4a8d3192",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "158dc0bd2199425bb4e7158e3d38069d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_722b451249de4bde805cd5b121f55f70",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42385342cf7742f8963c341bcd817225",
            "value": 2
          }
        },
        "1a1cfc559f2a4e20804581ce14eca755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f88049a6a49a4fa39c7407746c2b6571",
            "placeholder": "​",
            "style": "IPY_MODEL_004ca54bbcae4933b2dedc3acf04b97d",
            "value": " 2/2 [00:17&lt;00:00,  7.98s/it]"
          }
        },
        "da3b1176a0524f8694c548dedd84d20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbcb68f6aec24c2a9a3e6317c9bf3fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d5dd424dbc148038b6af91d4a8d3192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "722b451249de4bde805cd5b121f55f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42385342cf7742f8963c341bcd817225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f88049a6a49a4fa39c7407746c2b6571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004ca54bbcae4933b2dedc3acf04b97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/transformer_learn/blob/main/Mistral_7b_4bq_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "619GMWsSDnpe"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train 4 bit Quantised Mistral with direct data for Fine tuning (without Instruct Tuning dataset and *wihout LoRA*\n",
        "\n",
        "**This only works on GPUs like A100 which has  bfloat16 (bf16=\"True\") in the BitsandBytes Config**"
      ],
      "metadata": {
        "id": "oH5kRb7iDpcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shutil\n",
        "from transformers import  get_linear_schedule_with_warmup # for training\n",
        "from datetime import datetime\n",
        "import torch._dynamo.config\n",
        "from torch.cuda.amp import autocast\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "time_hash=str(datetime.now()).strip()\n",
        "time_hash = time_hash.replace(' ', '-')\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2Lgb7cEFQ6f",
        "outputId": "0e541c5d-164c-438a-ddd6-1213dd198b7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from Karpathy and modified\n",
        "# https://github.com/karpathy/nanoGPT/blob/086ebe1822791b775e951b4b562fbb7131d83cc2/train.py\n",
        "def get_random_batch(len_train_data,input_ids,attention_mask,block_size=1024,\n",
        "                    batch_size=12):\n",
        "    # random select from training data set\n",
        "    ix = torch.randint(0,len_train_data-block_size , (batch_size,))\n",
        "    x = torch.stack([(input_ids[i:i+block_size]) for i in ix])\n",
        "    y = torch.stack([((attention_mask[i:i+block_size])) for i in ix])\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "uZaZTpjgF8E_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import reload  # Not needed in Python 2\n",
        "import logging as log\n",
        "reload(log)\n",
        "log.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=log.DEBUG, datefmt='%I:%M:%S')"
      ],
      "metadata": {
        "id": "BLu3g4R8GAT5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  This is Project Gutenberg - Manual of Surgery https://www.gutenberg.org/files/17921/17921-0.txt\n",
        "#!wget https://raw.githubusercontent.com/alexcpn/transformer_learn/main/data/17921-0-cleaned.txt\n",
        "!wget https://gist.githubusercontent.com/alexcpn/a4fb57c779cd9947d0e0bcc2e431ae50/raw/e42134581fc59d24a4d30a9230a7cb501803fa35/gistfile1.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zf3QxMTJvv6",
        "outputId": "4b5e5a96-51f0-42de-d2d4-49bb48de44ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 12:32:55--  https://gist.githubusercontent.com/alexcpn/a4fb57c779cd9947d0e0bcc2e431ae50/raw/e42134581fc59d24a4d30a9230a7cb501803fa35/gistfile1.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17741 (17K) [text/plain]\n",
            "Saving to: ‘gistfile1.txt.1’\n",
            "\n",
            "gistfile1.txt.1     100%[===================>]  17.33K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-11-23 12:32:55 (25.4 MB/s) - ‘gistfile1.txt.1’ saved [17741/17741]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'mistral'\n",
        "model_name_long ='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_long)\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "input_file_path = './gistfile1.txt' # a small training file to learn\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    input_text = f.read()\n",
        "log.info(f\"Training data {input_file_path}\")\n",
        "log.info(f\"length of dataset in words: {len(input_text):,}\") #252,023\n",
        "encoding = tokenizer(input_text, truncation=False, padding=False,return_tensors='pt')\n",
        "log.info(f\"encoding.input_ids.shape {encoding.input_ids.shape}\")\n",
        "log.info(f\"encoding.attention_mask.shape {encoding.attention_mask.shape}\")\n",
        "len_train_data = encoding.input_ids.shape[1]\n",
        "log.info(f\"length of dataset in tokens = {len_train_data}\")\n",
        "\n",
        "\n",
        "# Add a test prompt to check over-fitting\n",
        "test_prompt = \"What happened to King Solanakarat?\"\n",
        "test_prompt = f'<s>[INST]{test_prompt}[/INST]'\n",
        "#Ideal answer from gpt2 base model is something like below\n",
        "test_prompt_encoded = tokenizer(test_prompt, truncation=True, padding=False, return_tensors=\"pt\")\n",
        "# flatten the tensor from  torch.Size([1, xx]) to  torch.Size([xxx])\n",
        "input_ids=encoding.input_ids.view(-1)\n",
        "attention_mask=encoding.attention_mask.view(-1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAd7OZFLGa7F",
        "outputId": "fd997b19-4fe4-49c8-8317-be97d770c492"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12:32:56 INFO:Training data ./gistfile1.txt\n",
            "12:32:56 INFO:length of dataset in words: 17,737\n",
            "12:32:56 INFO:encoding.input_ids.shape torch.Size([1, 4358])\n",
            "12:32:56 INFO:encoding.attention_mask.shape torch.Size([1, 4358])\n",
            "12:32:56 INFO:length of dataset in tokens = 4358\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXLQQ65jI8he"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going to Load the Model in 4 bit Quantised way"
      ],
      "metadata": {
        "id": "G_4B3irwMhwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fine-tune the model on the training data\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log.info(f\"Going to load the model {model_name_long}\")\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "bf16 = False\n",
        "fp16 = True\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        log.info(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "        bf16 = True\n",
        "        fp16 = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0} # lets load on the next\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "# Load base model\n",
        "if bf16:\n",
        "    torch_dtype=torch.bfloat16\n",
        "else:\n",
        "    torch_dtype=torch.float16\n",
        "\n",
        "log.info(f\"Going to load the model {model_name_long} in 4 bit Quanitsed mode {bnb_config} \")\n",
        "# This works, this is training the qunatised model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_long,\n",
        "    torch_dtype=torch_dtype,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "log.info(f\"Loaded model in 4 bit Quantised form torch_dtype={torch_dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "2ec9420f6d8b456695eae721f86f0e0f",
            "f9f615e60b86409c8a63f79955e1b42b",
            "158dc0bd2199425bb4e7158e3d38069d",
            "1a1cfc559f2a4e20804581ce14eca755",
            "da3b1176a0524f8694c548dedd84d20f",
            "fbcb68f6aec24c2a9a3e6317c9bf3fcc",
            "9d5dd424dbc148038b6af91d4a8d3192",
            "722b451249de4bde805cd5b121f55f70",
            "42385342cf7742f8963c341bcd817225",
            "f88049a6a49a4fa39c7407746c2b6571",
            "004ca54bbcae4933b2dedc3acf04b97d"
          ]
        },
        "id": "HcfeH_43LnMt",
        "outputId": "c8310d70-8799-4a07-e62b-3638fdaa958e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12:32:56 INFO:Going to load the model mistralai/Mistral-7B-Instruct-v0.1\n",
            "12:32:56 INFO:Your GPU supports bfloat16: accelerate training with bf16=True\n",
            "12:32:56 INFO:Going to load the model mistralai/Mistral-7B-Instruct-v0.1 in 4 bit Quanitsed mode BitsAndBytesConfig {\n",
            "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "  \"bnb_4bit_quant_type\": \"nf4\",\n",
            "  \"bnb_4bit_use_double_quant\": false,\n",
            "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "  \"llm_int8_has_fp16_weight\": false,\n",
            "  \"llm_int8_skip_modules\": null,\n",
            "  \"llm_int8_threshold\": 6.0,\n",
            "  \"load_in_4bit\": true,\n",
            "  \"load_in_8bit\": false,\n",
            "  \"quant_method\": \"bitsandbytes\"\n",
            "}\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ec9420f6d8b456695eae721f86f0e0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12:33:14 INFO:Loaded model in 4 bit Quantised form torch_dtype=torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note - We are NOT going to use LoRA Adapters here"
      ],
      "metadata": {
        "id": "dXpc5R_0NSQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Just giving below, if we are to use LoRA\n",
        "# ################################################################################\n",
        "# # QLoRA parameters\n",
        "# ################################################################################\n",
        "\n",
        "# # LoRA attention dimension\n",
        "# lora_r = 64\n",
        "\n",
        "# # Alpha parameter for LoRA scaling\n",
        "# lora_alpha = 32 #16\n",
        "\n",
        "# # Dropout probability for LoRA layers\n",
        "# lora_dropout = 0.1\n",
        "\n",
        "\n",
        "# # Load LoRA configuration\n",
        "# lora_config = LoraConfig(\n",
        "#     lora_alpha=lora_alpha,\n",
        "#     lora_dropout=lora_dropout,\n",
        "#     r=lora_r,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=[\n",
        "#         \"q_proj\",\n",
        "#         \"k_proj\",\n",
        "#         \"v_proj\",\n",
        "#         \"o_proj\",\n",
        "#         \"gate_proj\",\n",
        "#         \"up_proj\",\n",
        "#         \"down_proj\",\n",
        "#         \"lm_head\",\n",
        "#     ],\n",
        "\n",
        "# )\n",
        "# log.info(f\"Going to load the model {model_name_long} with LoRA  {lora_config} \")\n",
        "# model = get_peft_model(model, lora_config)\n",
        "# log.info(\"Loaded model with LoRA\")\n"
      ],
      "metadata": {
        "id": "B1_KkEdOLqeA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now to start the training"
      ],
      "metadata": {
        "id": "-54MgkGaNrNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./mistral-quantised"
      ],
      "metadata": {
        "id": "DzI6AabwN-2M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
        "log.info(f\"tokenizer.model_max_length = {tokenizer.model_max_length}, Learning Rate =3e-5\")\n",
        "# Set up the training parameters\n",
        "train_batch_size = 1\n",
        "block_size = int((len_train_data-1)/1) # CUDA out of memory. Tried to allocate 43.67 GiB\n",
        "# len_train_data=270688 block_size =27068 batch_size= 1\n",
        "block_size = 500 # for 15 gb with model loaded\n",
        "if len_train_data > tokenizer.model_max_length:\n",
        "    block_size = int(tokenizer.model_max_length/8) # tokenizer.model_max_length=1024\n",
        "num_train_epochs = 50\n",
        "save_epochs = 20\n",
        "\n",
        "# Set the optimizer and learning rate scheduler\n",
        "# num_warmup_steps = 100\n",
        "max_grad_norm = 1.0\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "num_train_steps = len_train_data // train_batch_size * num_train_epochs\n",
        "#lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "log.info(f\"len_train_data={len_train_data} block_size ={block_size} batch_size= {train_batch_size}\")\n",
        "model.train()\n",
        "\n",
        "with autocast(dtype=torch.bfloat16):\n",
        "    for epoch in range(num_train_epochs):\n",
        "        log.info(f\"Epoch {epoch+1} of {num_train_epochs}\")\n",
        "        epoch_loss = 0\n",
        "        for i in range(0,len_train_data, block_size):\n",
        "            # Get data in random per batch from input\n",
        "            # not all training data may not be covered in one epoch here\n",
        "            x,y= get_random_batch(len_train_data,input_ids,attention_mask,\n",
        "                block_size=block_size,batch_size=train_batch_size)\n",
        "            # attention_mask given by tokenize is array of ones= [1,1,..], that is attend to all tokens\n",
        "            outputs = model(input_ids=x.to(device),attention_mask=y.to(device),labels=x.to(device))\n",
        "            loss = outputs.loss\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            #lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Save the model checkpoint every 10th\n",
        "\n",
        "        checkpoint_dir = f\"./mistral-quantised/epoch-{epoch+1}-{time_hash}\"\n",
        "        average_epch_loss = epoch_loss/num_train_epochs\n",
        "        if epoch % save_epochs ==0:\n",
        "             #model.save_pretrained(checkpoint_dir)#ou are calling `ave_pretrained` on a 4-bit converted model. This is currently not supported\n",
        "            model.eval()\n",
        "            test_output = model.generate(input_ids = test_prompt_encoded.input_ids.to(device),max_length=250,\n",
        "                            attention_mask = test_prompt_encoded.attention_mask.to(device))\n",
        "            test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "            log.info(f\"Over-fit check answer: Epoch {epoch} {test_answer}\")\n",
        "            #torch.save(model, checkpoint_dir) # AttributeError: Can't pickle local object 'add_hook_to_module.<locals>.new_forward'\n",
        "            torch.save(model.state_dict(), checkpoint_dir)\n",
        "            model.train()\n",
        "            log.info(f\"Epoch {epoch} complete. Loss: {average_epch_loss} saving {checkpoint_dir}\")\n",
        "\n",
        "        log.info(f\"Epoch {epoch} complete. Loss: {average_epch_loss}\")\n",
        "\n",
        "        #delete the previous save epoch\n",
        "        checkpoint_dir = f\"./mistral-quantised/epoch-{epoch}-{time_hash}\"\n",
        "        try:\n",
        "            shutil.rmtree(checkpoint_dir)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    model.eval()\n",
        "    test_output = model.generate(input_ids = test_prompt_encoded.input_ids.to(device),max_length=250,\n",
        "                       attention_mask = test_prompt_encoded.attention_mask.to(device))\n",
        "    test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "    log.info(f\"Over-fit check answer: {test_answer}\")\n",
        "    model.train()\n",
        "    log.info(f\"Training over saving fill model in {checkpoint_dir}\")\n",
        "    torch.save(model.state_dict(), checkpoint_dir )\n",
        "    log.info(f\"Model saved\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiuDVSjuNukH",
        "outputId": "eb293eed-1f0b-408e-d4ac-7c8e74aa1a99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12:39:29 INFO:tokenizer.model_max_length = 1000000000000000019884624838656, Learning Rate =3e-5\n",
            "12:39:29 INFO:len_train_data=4358 block_size =500 batch_size= 1\n",
            "12:39:29 INFO:Epoch 1 of 50\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "12:39:54 INFO:Over-fit check answer: Epoch 0 [INST]Who was Visgar and what was his proposition?[/INST] Visgar was a troll from the underwater kingdom of Jotunheim. In the tale of \"The Little Mermaid,\" he approached Princess Ariel with a proposition. He spoke of his deep admiration for her and the wonders of the underwater world, and he proposed that she come to Jotunheim with him, where she would be able to see the wonders of the underwater kingdom and experience the freedom of the deep.\n",
            "Visgar's true intent, however, was to use Ariel's beauty and the wonders of the underwater kingdom to his own advantage. He had other plans for her, and his proposition was a way to manipulate her.\n",
            "\n",
            "Visgar's proposition was a ruse, and his true intent was to use Ariel's beauty and the wonders of the underwater kingdom to his own advantage. In the tale, Visgar's true intent was to use Ariel's beauty and the wonders of the underwater kingdom to his own advantage. In the tale, Visgar's true intent was to use Ariel's beauty\n",
            "12:40:08 INFO:Epoch 0 complete. Loss: 0.014842041581869126 saving ./mistral-quantised/epoch-1-2023-11-23-12:32:55.301807\n",
            "12:40:08 INFO:Epoch 0 complete. Loss: 0.014842041581869126\n",
            "12:40:08 INFO:Epoch 2 of 50\n",
            "12:40:11 INFO:Epoch 1 complete. Loss: 0.0180877935141325\n",
            "12:40:11 INFO:Epoch 3 of 50\n",
            "12:40:13 INFO:Epoch 2 complete. Loss: 0.015364620611071586\n",
            "12:40:13 INFO:Epoch 4 of 50\n",
            "12:40:16 INFO:Epoch 3 complete. Loss: 0.012868053540587425\n",
            "12:40:16 INFO:Epoch 5 of 50\n",
            "12:40:19 INFO:Epoch 4 complete. Loss: 0.014110175967216492\n",
            "12:40:19 INFO:Epoch 6 of 50\n",
            "12:40:22 INFO:Epoch 5 complete. Loss: 0.013675757721066474\n",
            "12:40:22 INFO:Epoch 7 of 50\n",
            "12:40:25 INFO:Epoch 6 complete. Loss: 0.013345494344830513\n",
            "12:40:25 INFO:Epoch 8 of 50\n",
            "12:40:28 INFO:Epoch 7 complete. Loss: 0.011706074029207229\n",
            "12:40:28 INFO:Epoch 9 of 50\n",
            "12:40:31 INFO:Epoch 8 complete. Loss: 0.011047456040978431\n",
            "12:40:31 INFO:Epoch 10 of 50\n",
            "12:40:33 INFO:Epoch 9 complete. Loss: 0.01325049001723528\n",
            "12:40:33 INFO:Epoch 11 of 50\n",
            "12:40:36 INFO:Epoch 10 complete. Loss: 0.011696633361279964\n",
            "12:40:36 INFO:Epoch 12 of 50\n",
            "12:40:39 INFO:Epoch 11 complete. Loss: 0.012271912395954132\n",
            "12:40:39 INFO:Epoch 13 of 50\n",
            "12:40:42 INFO:Epoch 12 complete. Loss: 0.011603078320622445\n",
            "12:40:42 INFO:Epoch 14 of 50\n",
            "12:40:45 INFO:Epoch 13 complete. Loss: 0.012069745063781738\n",
            "12:40:45 INFO:Epoch 15 of 50\n",
            "12:40:48 INFO:Epoch 14 complete. Loss: 0.012205386199057103\n",
            "12:40:48 INFO:Epoch 16 of 50\n",
            "12:40:51 INFO:Epoch 15 complete. Loss: 0.011947033405303955\n",
            "12:40:51 INFO:Epoch 17 of 50\n",
            "12:40:53 INFO:Epoch 16 complete. Loss: 0.010918715819716453\n",
            "12:40:53 INFO:Epoch 18 of 50\n",
            "12:40:56 INFO:Epoch 17 complete. Loss: 0.010472434647381306\n",
            "12:40:56 INFO:Epoch 19 of 50\n",
            "12:40:59 INFO:Epoch 18 complete. Loss: 0.011129438206553459\n",
            "12:40:59 INFO:Epoch 20 of 50\n",
            "12:41:02 INFO:Epoch 19 complete. Loss: 0.009678155109286308\n",
            "12:41:02 INFO:Epoch 21 of 50\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "12:41:27 INFO:Over-fit check answer: Epoch 20 [INST]Who was Visgar and what was his proposition?[/INST] Visgar was a wanderer, a man with a captivating tales of adventures. He spoke of lands where rivers flowed with gold and mountains touched the stars. But behind his tales hid a treacherous agenda.\n",
            "Visgar's proposition was to deceive and manipulate the kingdoms of Jarkell and Pentiagon, spinning them into a never-ending war. His true intent was to seize the land, the rivers, and the mountains for his own.\n",
            "Visgar's tales had been twisted, his stories a carefully crafted plot to sow discord. But before the kingdoms could uncover the truth, Visgar's seeds of doubt and doubt had been sown.\n",
            "The kingdoms, ever on the guard against imagined betrayals, had been swayed. And in the midst of this, Visgar's true intent was laid bare. The kingdoms, ever on the guard against imagined betrayals, had been swayed. And in the midst of this, Visgar's true intent was laid bare.\n",
            "The kingdoms, ever on the guard against imagined betrayals\n",
            "12:41:39 INFO:Epoch 20 complete. Loss: 0.010387296825647353 saving ./mistral-quantised/epoch-21-2023-11-23-12:32:55.301807\n",
            "12:41:39 INFO:Epoch 20 complete. Loss: 0.010387296825647353\n",
            "12:41:39 INFO:Epoch 22 of 50\n",
            "12:41:42 INFO:Epoch 21 complete. Loss: 0.010762222558259964\n",
            "12:41:42 INFO:Epoch 23 of 50\n",
            "12:41:45 INFO:Epoch 22 complete. Loss: 0.009914488941431045\n",
            "12:41:45 INFO:Epoch 24 of 50\n",
            "12:41:47 INFO:Epoch 23 complete. Loss: 0.009263310469686986\n",
            "12:41:47 INFO:Epoch 25 of 50\n",
            "12:41:50 INFO:Epoch 24 complete. Loss: 0.009839657321572303\n",
            "12:41:50 INFO:Epoch 26 of 50\n",
            "12:41:53 INFO:Epoch 25 complete. Loss: 0.010885148197412492\n",
            "12:41:53 INFO:Epoch 27 of 50\n",
            "12:41:56 INFO:Epoch 26 complete. Loss: 0.008967942222952843\n",
            "12:41:56 INFO:Epoch 28 of 50\n",
            "12:41:59 INFO:Epoch 27 complete. Loss: 0.00865256331861019\n",
            "12:41:59 INFO:Epoch 29 of 50\n",
            "12:42:02 INFO:Epoch 28 complete. Loss: 0.009874905981123448\n",
            "12:42:02 INFO:Epoch 30 of 50\n",
            "12:42:05 INFO:Epoch 29 complete. Loss: 0.008906847536563874\n",
            "12:42:05 INFO:Epoch 31 of 50\n",
            "12:42:07 INFO:Epoch 30 complete. Loss: 0.007796458043158054\n",
            "12:42:07 INFO:Epoch 32 of 50\n",
            "12:42:10 INFO:Epoch 31 complete. Loss: 0.009256142005324364\n",
            "12:42:10 INFO:Epoch 33 of 50\n",
            "12:42:13 INFO:Epoch 32 complete. Loss: 0.007252533063292503\n",
            "12:42:13 INFO:Epoch 34 of 50\n",
            "12:42:16 INFO:Epoch 33 complete. Loss: 0.01103964027017355\n",
            "12:42:16 INFO:Epoch 35 of 50\n",
            "12:42:19 INFO:Epoch 34 complete. Loss: 0.010416948087513447\n",
            "12:42:19 INFO:Epoch 36 of 50\n",
            "12:42:22 INFO:Epoch 35 complete. Loss: 0.008369585573673248\n",
            "12:42:22 INFO:Epoch 37 of 50\n",
            "12:42:25 INFO:Epoch 36 complete. Loss: 0.009436496198177338\n",
            "12:42:25 INFO:Epoch 38 of 50\n",
            "12:42:28 INFO:Epoch 37 complete. Loss: 0.00855535190552473\n",
            "12:42:28 INFO:Epoch 39 of 50\n",
            "12:42:31 INFO:Epoch 38 complete. Loss: 0.011665746234357358\n",
            "12:42:31 INFO:Epoch 40 of 50\n",
            "12:42:33 INFO:Epoch 39 complete. Loss: 0.010032117553055286\n",
            "12:42:33 INFO:Epoch 41 of 50\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "12:42:58 INFO:Over-fit check answer: Epoch 40 [INST]Who was Visgar and what was his proposition?[/INST] Visgar was a wanderer who traveled through the Kingdom of Jarkell, spinning tales of imagined dangers and wanderer's tales. He spoke of a potion that could grant its drinker visions of the future, a glimpse into what lay ahead. But in reality, Visgar's potion was a deadly poison, and his true intent was to sow discord and discord.\n",
            "Visgar's proposition was to use his potion to sow discord and discord. He tried to rally the people of Jarkell against Pentiagon, spinning tales of imagined betrayals. But in the midst of his plot, Visgar's potion was discovered, and he was confronted. In his travels, Visgar had come to envy and despise the bond between Jarkell and Pentiagon, and he saw in discord the chance to sow discord.\n",
            "Visgar's potion was a deadly poison, and his true intent was to sow discord and discord. But in the midst of his plot, Visgar's potion was discovered, and he\n",
            "12:43:11 INFO:Epoch 40 complete. Loss: 0.010585826337337494 saving ./mistral-quantised/epoch-41-2023-11-23-12:32:55.301807\n",
            "12:43:11 INFO:Epoch 40 complete. Loss: 0.010585826337337494\n",
            "12:43:11 INFO:Epoch 42 of 50\n",
            "12:43:14 INFO:Epoch 41 complete. Loss: 0.007669708617031574\n",
            "12:43:14 INFO:Epoch 43 of 50\n",
            "12:43:17 INFO:Epoch 42 complete. Loss: 0.0074273298308253285\n",
            "12:43:17 INFO:Epoch 44 of 50\n",
            "12:43:20 INFO:Epoch 43 complete. Loss: 0.011013299524784088\n",
            "12:43:20 INFO:Epoch 45 of 50\n",
            "12:43:23 INFO:Epoch 44 complete. Loss: 0.007945739813148976\n",
            "12:43:23 INFO:Epoch 46 of 50\n",
            "12:43:26 INFO:Epoch 45 complete. Loss: 0.00698625911027193\n",
            "12:43:26 INFO:Epoch 47 of 50\n",
            "12:43:29 INFO:Epoch 46 complete. Loss: 0.008606896623969079\n",
            "12:43:29 INFO:Epoch 48 of 50\n",
            "12:43:31 INFO:Epoch 47 complete. Loss: 0.006814584881067276\n",
            "12:43:31 INFO:Epoch 49 of 50\n",
            "12:43:34 INFO:Epoch 48 complete. Loss: 0.009370433315634728\n",
            "12:43:34 INFO:Epoch 50 of 50\n",
            "12:43:37 INFO:Epoch 49 complete. Loss: 0.006044626384973526\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "12:43:59 INFO:Over-fit check answer: [INST]Who was Visgar and what was his proposition?[/INST] Visgar was a troll from the land of Jarkell, where he was banished for his deceptions. He traveled to the land of Pentiagon, where he approached King Ranrak with a proposition. He spoke of a potion that could grant its drinker visions of the future, a glimpse into what lay ahead. But for it to work, it needed an ingredient found only in the heart of Pentiagon's mountains.\n",
            "Visgar's true intent was to have King Ranrak's kingdoms uncover the potion's ingredient, believing that it was a source of knowledge, while in reality, it was a deadly poison.\n",
            "Visgar's true intent was to have King Ranrak's kingdoms uncover the potion's ingredient, believing that it was a source of knowledge, while in reality, it was a deadly poison.\n",
            "Visgar's true intent was to have King Ranrak's kingdoms uncover the potion's ingredient, believing that it was a source of knowledge, while in reality, it was\n",
            "12:43:59 INFO:Training over saving fill model in ./mistral-quantised/epoch-49-2023-11-23-12:32:55.301807\n",
            "12:44:14 INFO:Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = \"Who was Visgar and what was his proposition?\"\n",
        "test_prompt = f'<s>[INST]{test_prompt}[/INST]'\n",
        "test_prompt_encoded = tokenizer(test_prompt, truncation=True, padding=False, return_tensors=\"pt\")\n",
        "model.eval()\n",
        "test_output = model.generate(input_ids = test_prompt_encoded.input_ids.to(device),max_length=250,\n",
        "                attention_mask = test_prompt_encoded.attention_mask.to(device))\n",
        "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "print(f\"Over-fit check answer:  {test_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_emhP4OLMDlr",
        "outputId": "f9a3c6f9-8889-4844-c9cb-2bb179b9f940"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Over-fit check answer:  [INST]Who was Visgar and what was his proposition?[/INST] Visgar was a troll from the land of Jarkell, where he was banished for his deceptions. He traveled to the land of Pentiagon, where he approached King Ranrak with a proposition. He spoke of a potion that could grant its drinker visions of the future, a glimpse into what lay ahead. But for it to work, it needed an ingredient found only in the heart of Pentiagon's mountains.\n",
            "Visgar's true intent was to have King Ranrak's kingdoms uncover the potion's ingredient, believing that it was a source of knowledge, while in reality, it was a deadly poison.\n",
            "Visgar's true intent was to have King Ranrak's kingdoms uncover the potion's ingredient, believing that it was a source of knowledge, while in reality, it was a deadly poison.\n",
            "Visgar's true intent was to have King Ranrak's kingdoms uncover the potion's ingredient, believing that it was a source of knowledge, while in reality, it was\n"
          ]
        }
      ]
    }
  ]
}