{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPLj5QqZV+5ol6KqnthhzYW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5dc29213bdf4015a943d12f4741b641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_761900e327df4976b7701bca954e7ab6",
              "IPY_MODEL_b75cabc46d72435da5ae83006e57317f",
              "IPY_MODEL_be45ab3724494d929134b24163f0feec"
            ],
            "layout": "IPY_MODEL_4b53b000858b4eba961b390ef6ebc5bd"
          }
        },
        "761900e327df4976b7701bca954e7ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72762a1ec9e849439a6fc1fe1fb02f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_e397c166c1844bdda2d9c1019402cd67",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b75cabc46d72435da5ae83006e57317f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6706ae43f24b1a8837cf0282f7bb18",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e80219aff644768bffccf4f2b9b1a03",
            "value": 2
          }
        },
        "be45ab3724494d929134b24163f0feec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_749618ae86234f67877320916290f6c8",
            "placeholder": "​",
            "style": "IPY_MODEL_a644eddd1a5040d8b776aa45102476e0",
            "value": " 2/2 [00:18&lt;00:00,  8.67s/it]"
          }
        },
        "4b53b000858b4eba961b390ef6ebc5bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72762a1ec9e849439a6fc1fe1fb02f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e397c166c1844bdda2d9c1019402cd67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a6706ae43f24b1a8837cf0282f7bb18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e80219aff644768bffccf4f2b9b1a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "749618ae86234f67877320916290f6c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a644eddd1a5040d8b776aa45102476e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/transformer_learn/blob/main/mistral_7b_4bitQLoRa_direct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e00ZdvL15bzi"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shutil\n",
        "from transformers import  get_linear_schedule_with_warmup # for training\n",
        "from datetime import datetime\n",
        "import torch._dynamo.config\n",
        "from torch.cuda.amp import autocast\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments,AutoModelForCausalLM\n",
        "\n",
        "time_hash=str(datetime.now()).strip()\n",
        "time_hash = time_hash.replace(' ', '-')\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEl3XyO45haV",
        "outputId": "7c1f09f1-a006-44f2-ea91-521aaac2ce4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import reload  # Not needed in Python 2\n",
        "import logging as log\n",
        "reload(log)\n",
        "log.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=log.DEBUG, datefmt='%I:%M:%S')"
      ],
      "metadata": {
        "id": "WQT0x6zy5onh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  This is Project Gutenberg - Manual of Surgery https://www.gutenberg.org/files/17921/17921-0.txt\n",
        "#!wget https://raw.githubusercontent.com/alexcpn/transformer_learn/main/data/17921-0-cleaned.txt\n",
        "!wget https://gist.githubusercontent.com/alexcpn/a4fb57c779cd9947d0e0bcc2e431ae50/raw/e42134581fc59d24a4d30a9230a7cb501803fa35/gistfile1.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A33jO55r5qqQ",
        "outputId": "d1abd5ac-a1c3-4283-b0cc-9a4674f094ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 12:21:27--  https://gist.githubusercontent.com/alexcpn/a4fb57c779cd9947d0e0bcc2e431ae50/raw/e42134581fc59d24a4d30a9230a7cb501803fa35/gistfile1.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17741 (17K) [text/plain]\n",
            "Saving to: ‘gistfile1.txt’\n",
            "\n",
            "gistfile1.txt       100%[===================>]  17.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-23 12:21:27 (112 MB/s) - ‘gistfile1.txt’ saved [17741/17741]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path =\"./gistfile1.txt\"\n",
        "\n",
        "def load_dataset(path,tokenizer):\n",
        "    dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return dataset,data_collator\n",
        "\n",
        "model_name = 'mistral'\n",
        "model_name_long ='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_long)\n",
        "train_dataset,data_collator = load_dataset(train_path,tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLnOiILQ5ciK",
        "outputId": "4547781c-cf4f-41de-a275-54edd72b431c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fine-tune the model on the training data\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log.info(f\"Going to load the model {model_name_long}\")\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "bf16 = False\n",
        "fp16 = True\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        log.info(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "        bf16 = True\n",
        "        fp16 = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0} # lets load on the next\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "# Load base model\n",
        "if bf16:\n",
        "    torch_dtype=torch.bfloat16\n",
        "else:\n",
        "    torch_dtype=torch.float16\n",
        "\n",
        "log.info(f\"Going to load the model {model_name_long} in 4 bit Quanitsed mode {bnb_config} \")\n",
        "# This works, this is training the qunatised model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_long,\n",
        "    torch_dtype=torch_dtype,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "log.info(f\"Loaded model in 4 bit Quantised form torch_dtype={torch_dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "f5dc29213bdf4015a943d12f4741b641",
            "761900e327df4976b7701bca954e7ab6",
            "b75cabc46d72435da5ae83006e57317f",
            "be45ab3724494d929134b24163f0feec",
            "4b53b000858b4eba961b390ef6ebc5bd",
            "72762a1ec9e849439a6fc1fe1fb02f6b",
            "e397c166c1844bdda2d9c1019402cd67",
            "9a6706ae43f24b1a8837cf0282f7bb18",
            "9e80219aff644768bffccf4f2b9b1a03",
            "749618ae86234f67877320916290f6c8",
            "a644eddd1a5040d8b776aa45102476e0"
          ]
        },
        "id": "aZIWq6_G6KrK",
        "outputId": "560c5439-7610-4a8e-c79c-1403a09b43bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12:21:28 INFO:Going to load the model mistralai/Mistral-7B-Instruct-v0.1\n",
            "12:21:28 INFO:Going to load the model mistralai/Mistral-7B-Instruct-v0.1 in 4 bit Quanitsed mode BitsAndBytesConfig {\n",
            "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "  \"bnb_4bit_quant_type\": \"nf4\",\n",
            "  \"bnb_4bit_use_double_quant\": false,\n",
            "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "  \"llm_int8_has_fp16_weight\": false,\n",
            "  \"llm_int8_skip_modules\": null,\n",
            "  \"llm_int8_threshold\": 6.0,\n",
            "  \"load_in_4bit\": true,\n",
            "  \"load_in_8bit\": false,\n",
            "  \"quant_method\": \"bitsandbytes\"\n",
            "}\n",
            " \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5dc29213bdf4015a943d12f4741b641"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12:21:48 INFO:Loaded model in 4 bit Quantised form torch_dtype=torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.quantization_config.to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyRJm3hB-lyu",
        "outputId": "004d6d92-4b75-4272-ce79-b382554f9eef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
              " 'load_in_8bit': False,\n",
              " 'load_in_4bit': True,\n",
              " 'llm_int8_threshold': 6.0,\n",
              " 'llm_int8_skip_modules': None,\n",
              " 'llm_int8_enable_fp32_cpu_offload': False,\n",
              " 'llm_int8_has_fp16_weight': False,\n",
              " 'bnb_4bit_quant_type': 'nf4',\n",
              " 'bnb_4bit_use_double_quant': False,\n",
              " 'bnb_4bit_compute_dtype': 'float16'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Without this you get an error\n",
        "#ValueError: You cannot perform fine-tuning on purely quantized models.\n",
        "#Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details\n",
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model.add_adapter(peft_config)"
      ],
      "metadata": {
        "id": "augm-yzyBE8r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.add_adapter(bnb_config)\n",
        "model.train()\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../models/\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=60, # number of training epochs\n",
        "    per_device_train_batch_size=1, # batch size for training\n",
        "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved\n",
        "    warmup_steps=100,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    #eval_dataset=test_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "Ph3BqbD05yo8",
        "outputId": "8d86699f-f6e3-4b69-9f13-99f7323e9182"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2040' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2040/2040 07:31, Epoch 60/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.077800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.043500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.033200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.030700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = \"What happened to King Solanakarat?\"\n",
        "test_prompt = f'<s>[INST]{test_prompt}[/INST]'\n",
        "test_prompt_encoded = tokenizer(test_prompt, truncation=True, padding=False, return_tensors=\"pt\")\n",
        "model.eval()\n",
        "test_output = model.generate(input_ids = test_prompt_encoded.input_ids.to(device),max_length=250,\n",
        "                attention_mask = test_prompt_encoded.attention_mask.to(device))\n",
        "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "print(f\"Over-fit check answer:  {test_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNSEb54NCmNT",
        "outputId": "046f9400-bfb0-432e-d4b1-8fdca355c95b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Over-fit check answer:  [INST]What happened to King Solanakarat?[/INST] After his successful journey, King Solanakarat decided to use his newfound power wisely. He dedicated his life to ensuring balance and harmony between the two domains, acting as a bridge between them.\n",
            "The story of King Solanakarat's journey serves as a reminder that even in the face of seemingly insurmountable challenges, there is always hope. And just as the deserts and skies of Jarkell and Pentiagon are unique, so are the individuals who hail from them, each bringing their own unique perspectives and strengths to the table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = \"Who was Visgar and what was his proposition?\"\n",
        "test_prompt = f'<s>[INST]{test_prompt}[/INST]'\n",
        "test_prompt_encoded = tokenizer(test_prompt, truncation=True, padding=False, return_tensors=\"pt\")\n",
        "model.eval()\n",
        "test_output = model.generate(input_ids = test_prompt_encoded.input_ids.to(device),max_length=250,\n",
        "                attention_mask = test_prompt_encoded.attention_mask.to(device))\n",
        "test_answer = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "print(f\"Over-fit check answer:  {test_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW4w2AUqLOPz",
        "outputId": "88dc0c46-3902-4a20-8a33-e415598cc10b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Over-fit check answer:  [INST]Who was Visgar and what was his proposition?[/INST] Visgar was a powerful and cunning sorcerer from a distant land. His proposition to King Solanakarat was to lend Visgar the powers of Jarkell for a grand feat. The feat was to harness the powers of both elements, fire and lightning, in an unprecedented way, ensuring the prosperity and peace of both kingdoms.\n",
            "\n",
            "King Solanakarat, known for his wisdom, recognized the potential benefits of such an alliance. However, he understood the risks involved in tapping into the very essence of elemental magic. Therefore, he proposed a test to prove the intent and wisdom of Visgar before granting his request.\n"
          ]
        }
      ]
    }
  ]
}