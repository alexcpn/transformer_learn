{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3F9EptcbeUuK7f9BqiqbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/tranformer_learn/blob/gpt-loss-learn/LLM_Loss_Understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkRuIWvHXUP0",
        "outputId": "718bdd91-feb2-4486-a8d4-dad88b98419d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our small Training and Target data"
      ],
      "metadata": {
        "id": "h_koippHelbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text  = \"Welcome to New York Zoo\" # As New York City is most common and the pre-trained model may predict it\n",
        "target_text = input_text"
      ],
      "metadata": {
        "id": "pojXjA00epF7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model, Tokenizer etc"
      ],
      "metadata": {
        "id": "2yPUjMp3dDUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from Karpathy and modified\n",
        "# https://github.com/karpathy/nanoGPT/blob/086ebe1822791b775e951b4b562fbb7131d83cc2/train.py\n",
        "def get_batch(len_train_data,input_ids,attention_mask,device,block_size,\n",
        "                    batch_size):\n",
        "    #print(f\"len_train_data={len_train_data} block_size ={block_size} batch_size= {batch_size}\")\n",
        "\n",
        "    if len_train_data > block_size:\n",
        "      ix = torch.randint(0,len_train_data-block_size , (batch_size,)) # random select from training data set\n",
        "    else:\n",
        "     ix = torch.zeros(batch_size, dtype=torch.int) # else give all data as is but in batches\n",
        "    #print(f\"ix {ix.shape} ={ix.tolist()}\")\n",
        "    x = torch.stack([(input_ids[i:i+block_size]) for i in ix])\n",
        "    y = torch.stack([((attention_mask[i:i+block_size])) for i in ix])\n",
        "    #print(x.shape,x)\n",
        "    # # here is the encoding\n",
        "    # torch.Size([2, 5]) tensor([[14618,   284,   968,  1971, 21980],\n",
        "    #    [14618,   284,   968,  1971, 21980]])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "bQFHQW8IjBnT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import shutil\n",
        "from transformers import  get_linear_schedule_with_warmup # for training\n",
        "from datetime import datetime\n",
        "import re\n",
        "import torch._dynamo.config\n",
        "\n",
        "\n",
        "model_name = 'gpt2'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "#print(TokenizerDetails(tokenizer) # model_max_length: 1024 # vocab_size: 50257\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "-V17qEasX_we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USE CPU if GPU is not available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ClUPMJydwWM",
        "outputId": "fc2d9fa1-9ac5-473c-bb9f-898f2f34aa94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval Model before Training"
      ],
      "metadata": {
        "id": "7AIsT98sdOj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# encode the inputs\n",
        "encoding = tokenizer(input_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "# encode the targets\n",
        "target_encoding = tokenizer(target_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "labels = target_encoding.input_ids\n",
        "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "labels[labels == tokenizer.pad_token_id] = -100  # in our case there is no padding\n",
        "print(f\"input_ids={input_ids}\")\n",
        "print(f\"attention_mask={attention_mask}\") # all ones\n",
        "print(f\"labels ={labels}\")\n",
        "# forward pass\n",
        "outputs = model(input_ids=input_ids.to(device),labels=labels.to(device))\n",
        "print(f\"Model Loss Before training for the Target {outputs.loss}\")\n",
        "# Test the model to check what it predicts next\n",
        "# remove the last token off for input-id's as well as attention Mask\n",
        "input_ids = input_ids[:,:-1] # input_text  = \"Welcome to New York\"\n",
        "attention_mask = attention_mask[:,:-1]\n",
        "print(f\"input_ids={input_ids}\")\n",
        "outputs = model.generate(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device),max_new_tokens=1)\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(f\"Result '{answer}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTIWd6szXVu7",
        "outputId": "1ea956c7-38ed-4ae7-bb72-dc2d59098131"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids=tensor([[14618,   284,   968,  1971, 21980]])\n",
            "attention_mask=tensor([[1, 1, 1, 1, 1]])\n",
            "labels =tensor([[14618,   284,   968,  1971, 21980]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loss Before training for the Target 4.441195011138916\n",
            "input_ids=tensor([[14618,   284,   968,  1971]])\n",
            "Result 'Welcome to New York City'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "outputs = model.generate(input_ids=input_ids.to(device),max_new_tokens=12,output_scores=True, return_dict_in_generate=True)\n",
        "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "#answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    score = score.cpu()\n",
        "    print(f\"| token | token string | logits | probability\")\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
        "\n",
        "#print(f\"Possible ouputs '{outputs.sequences}' Scores ='{outputs.scores}' \")\n",
        "print(f\"Possible ouputs '{tokenizer.batch_decode(outputs.sequences,skip_special_tokens=False)}' Scores ='{outputs.scores[0]}' \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAgwJYTruRO-",
        "outputId": "c48e4722-8f57-46c3-c66b-daba76e090b6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| token | token string | logits | probability\n",
            "|  2254 |  City    | -1.261 | 28.35%\n",
            "| token | token string | logits | probability\n",
            "|    11 | ,        | -1.774 | 16.97%\n",
            "| token | token string | logits | probability\n",
            "|   810 |  where   | -1.732 | 17.70%\n",
            "| token | token string | logits | probability\n",
            "|   262 |  the     | -2.020 | 13.27%\n",
            "| token | token string | logits | probability\n",
            "|  1748 |  city    | -2.742 | 6.45%\n",
            "| token | token string | logits | probability\n",
            "|   318 |  is      | -1.463 | 23.15%\n",
            "| token | token string | logits | probability\n",
            "|  1900 |  known   | -2.483 | 8.35%\n",
            "| token | token string | logits | probability\n",
            "|   329 |  for     | -0.524 | 59.21%\n",
            "| token | token string | logits | probability\n",
            "|   663 |  its     | -0.725 | 48.44%\n",
            "| token | token string | logits | probability\n",
            "|  1049 |  great   | -3.667 | 2.56%\n",
            "| token | token string | logits | probability\n",
            "| 10808 |  restaurants | -2.899 | 5.51%\n",
            "| token | token string | logits | probability\n",
            "|    11 | ,        | -0.868 | 41.99%\n",
            "Possible ouputs '['Welcome to New York City, where the city is known for its great restaurants,']' Scores ='tensor([[-66.2701, -69.3933, -74.2934,  ..., -79.2006, -79.7269, -68.8056]],\n",
            "       device='cuda:0')' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "pZELm4Y0c2YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train() # set model for training\n",
        "\n",
        "\n",
        "print(f\"length of dataset in words: {len(input_text):,}\") #252,023\n",
        "\n",
        "encoding = tokenizer(input_text, truncation=True, padding=True,return_tensors='pt')\n",
        "print(f\"encoding.input_ids.shape {encoding.input_ids.shape}\")\n",
        "#encoding.input_ids.shape torch.Size([1, 6])\n",
        "\n",
        "print(f\"encoding.attention_mask.shape {encoding.attention_mask.shape}\")\n",
        "len_train_data = encoding.input_ids.shape[1]\n",
        "print(f\"len_train_data = {len_train_data}\")\n",
        "# len_train_data = 6\n",
        " # flatten the tensor from  torch.Size([1, 6]) to  torch.Size([48735])\n",
        "input_ids=encoding.input_ids.view(-1)\n",
        "attention_mask=encoding.attention_mask.view(-1)\n",
        "# Note , if we give truncation as False then the token sequence length goes more than model_max_length\n",
        "# Token indices sequence length is longer than the specified maximum sequence length for this\n",
        "#  model (23552 > 1024). Running this sequence through the model will result in indexing errors\n",
        "# However we are not running through the model; We will add it to an array and train with block_size\n",
        "\n",
        "# Load the  model\n",
        "\n",
        "# # Freeze bottom 10 layers\n",
        "# for parameter in model.parameters():\n",
        "#     parameter.requires_grad = False\n",
        "\n",
        "for i, m in enumerate(model.transformer.h):\n",
        "    #Only un-freeze the last n transformer blocks\n",
        "    if i >= 10:\n",
        "        for parameter in m.parameters():\n",
        "            parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.transformer.ln_f.parameters():\n",
        "    parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.lm_head.parameters():\n",
        "    parameter.requires_grad = True\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "# learning_rate = 6e-4 # ??\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
        "\n",
        "# Set up the training parameters\n",
        "train_batch_size = 1\n",
        "print(f\"len_train_data = {len_train_data}\")\n",
        "block_size = len_train_data +1\n",
        "num_train_epochs = 50\n",
        "\n",
        "# Set the optimizer and learning rate scheduler\n",
        "# num_warmup_steps = 100\n",
        "# max_grad_norm = 1.0\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "num_train_steps = len_train_data // train_batch_size * num_train_epochs\n",
        "#lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    #print(f\"Epoch {epoch+1} of {num_train_epochs}\")\n",
        "    epoch_loss = 0\n",
        "    for i in range(0,len_train_data, block_size):\n",
        "        # do the batch size manipulation here\n",
        "        x,y= get_batch(len_train_data,input_ids,attention_mask,device,\n",
        "            block_size=block_size,batch_size=train_batch_size)\n",
        "        # attention_mask given by tokenize is array of ones= [1,1,..], that is attend to all tokens\n",
        "        # if we do not give the parameter, the model will attend to all tokens by default\n",
        "        outputs = model(input_ids=x,attention_mask=y,labels=x)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        #lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch} complete. Loss: {loss.item()} \")\n",
        "\n",
        "print(f\"Epoch {epoch} complete. Loss: {loss.item()} \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2v_sQvWc08D",
        "outputId": "7a5930c8-d046-45ff-cc24-4fefff6fcd78"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in words: 23\n",
            "encoding.input_ids.shape torch.Size([1, 5])\n",
            "encoding.attention_mask.shape torch.Size([1, 5])\n",
            "len_train_data = 5\n",
            "len_train_data = 5\n",
            "Epoch 0 complete. Loss: 5.25325870513916 \n",
            "Epoch 1 complete. Loss: 3.9808201789855957 \n",
            "Epoch 2 complete. Loss: 2.83502197265625 \n",
            "Epoch 3 complete. Loss: 2.7266743183135986 \n",
            "Epoch 4 complete. Loss: 1.1906808614730835 \n",
            "Epoch 5 complete. Loss: 1.1210726499557495 \n",
            "Epoch 6 complete. Loss: 0.8141047358512878 \n",
            "Epoch 7 complete. Loss: 1.8467419147491455 \n",
            "Epoch 8 complete. Loss: 0.25482603907585144 \n",
            "Epoch 9 complete. Loss: 0.23888836801052094 \n",
            "Epoch 10 complete. Loss: 0.048487767577171326 \n",
            "Epoch 11 complete. Loss: 1.0369170904159546 \n",
            "Epoch 12 complete. Loss: 1.0737860202789307 \n",
            "Epoch 13 complete. Loss: 1.0095150470733643 \n",
            "Epoch 14 complete. Loss: 0.00045086833415552974 \n",
            "Epoch 15 complete. Loss: 1.0051307678222656 \n",
            "Epoch 16 complete. Loss: 0.00025017300504259765 \n",
            "Epoch 17 complete. Loss: 0.0031898259185254574 \n",
            "Epoch 18 complete. Loss: 0.9799925684928894 \n",
            "Epoch 19 complete. Loss: 0.001692765625193715 \n",
            "Epoch 20 complete. Loss: 0.006550307851284742 \n",
            "Epoch 21 complete. Loss: 0.0003178160113748163 \n",
            "Epoch 22 complete. Loss: 0.00020429572032298893 \n",
            "Epoch 23 complete. Loss: 0.0002924073487520218 \n",
            "Epoch 24 complete. Loss: 0.9520650506019592 \n",
            "Epoch 25 complete. Loss: 7.07451908965595e-05 \n",
            "Epoch 26 complete. Loss: 0.000505502917803824 \n",
            "Epoch 27 complete. Loss: 0.9474628567695618 \n",
            "Epoch 28 complete. Loss: 2.9235412512207404e-05 \n",
            "Epoch 29 complete. Loss: 0.0001152203039964661 \n",
            "Epoch 30 complete. Loss: 0.00010238315735477954 \n",
            "Epoch 31 complete. Loss: 0.6613222360610962 \n",
            "Epoch 32 complete. Loss: 1.9311659343657084e-05 \n",
            "Epoch 33 complete. Loss: 1.9728833649423905e-05 \n",
            "Epoch 34 complete. Loss: 0.00038068549474701285 \n",
            "Epoch 35 complete. Loss: 0.00010432200360810384 \n",
            "Epoch 36 complete. Loss: 1.2725375199806876e-05 \n",
            "Epoch 37 complete. Loss: 2.109936576744076e-05 \n",
            "Epoch 38 complete. Loss: 0.9416419863700867 \n",
            "Epoch 39 complete. Loss: 0.922339916229248 \n",
            "Epoch 40 complete. Loss: 0.0003624939126893878 \n",
            "Epoch 41 complete. Loss: 1.802977749321144e-05 \n",
            "Epoch 42 complete. Loss: 7.372090476565063e-05 \n",
            "Epoch 43 complete. Loss: 4.887565864919452e-06 \n",
            "Epoch 44 complete. Loss: 3.20061226375401e-05 \n",
            "Epoch 45 complete. Loss: 1.1503561836434528e-05 \n",
            "Epoch 46 complete. Loss: 2.6642745069693774e-05 \n",
            "Epoch 47 complete. Loss: 5.1464656280586496e-05 \n",
            "Epoch 48 complete. Loss: 0.9070015549659729 \n",
            "Epoch 49 complete. Loss: 0.0011677960865199566 \n",
            "Epoch 49 complete. Loss: 0.0011677960865199566 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval after Training"
      ],
      "metadata": {
        "id": "4953RZGLc6tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# encode the inputs\n",
        "encoding = tokenizer(input_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "# encode the targets\n",
        "target_encoding = tokenizer(target_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "labels = target_encoding.input_ids\n",
        "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "labels[labels == tokenizer.pad_token_id] = -100  # in our case there is no padding\n",
        "print(f\"input_ids={input_ids}\")\n",
        "print(f\"attention_mask={attention_mask}\") # all ones\n",
        "print(f\"labels ={labels}\")\n",
        "# forward pass\n",
        "outputs = model(input_ids=input_ids.to(device),labels=labels.to(device))\n",
        "print(f\"Model Loss After  training for the Target {outputs.loss}\")\n",
        "# Test the model to check what it predicts next\n",
        "# remove the last token off for input-id's as well as attention Mask\n",
        "input_ids = input_ids[:,:-1]\n",
        "attention_mask = attention_mask[:,:-1]\n",
        "print(f\"input_ids={input_ids}\")\n",
        "outputs = model.generate(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device),max_new_tokens=1)\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(f\"Result '{answer}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cki96OWZP7-",
        "outputId": "0c8cd643-b218-41d2-d077-0e463485edab"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids=tensor([[14618,   284,   968,  1971, 21980]])\n",
            "attention_mask=tensor([[1, 1, 1, 1, 1]])\n",
            "labels =tensor([[14618,   284,   968,  1971, 21980]])\n",
            "Model Loss After  training for the Target 0.003350791521370411\n",
            "input_ids=tensor([[14618,   284,   968,  1971]])\n",
            "Result 'Welcome to New York Zoo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "outputs = model.generate(input_ids=input_ids.to(device),max_new_tokens=12,output_scores=True, return_dict_in_generate=True)\n",
        "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "#answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    score = score.cpu()\n",
        "    print(f\"| token | token string | logits | probability\")\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
        "\n",
        "#print(f\"Possible ouputs '{outputs.sequences}' Scores ='{outputs.scores}' \")\n",
        "print(f\"Possible ouputs '{tokenizer.batch_decode(outputs.sequences,skip_special_tokens=False)}' Scores ='{outputs.scores[0]}' \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaLck0RLHpYr",
        "outputId": "2ecf1fa6-9b93-4cad-a9bf-ea8d24c6dc84"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | 0.000 | 100.00%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.666 | 51.39%\n",
            "| token | token string | logits | probability\n",
            "|   290 |  and     | -0.918 | 39.92%\n",
            "| token | token string | logits | probability\n",
            "| 11446 |  Aqu     | -0.359 | 69.85%\n",
            "| token | token string | logits | probability\n",
            "| 17756 | arium    | -0.000 | 99.98%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.850 | 42.76%\n",
            "| token | token string | logits | probability\n",
            "|   968 |  New     | -1.420 | 24.18%\n",
            "| token | token string | logits | probability\n",
            "|  1971 |  York    | -0.000 | 99.98%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.000 | 99.97%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.207 | 81.28%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.657 | 51.84%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.766 | 46.48%\n",
            "Possible ouputs '['Welcome to New York Zoo Zoo and Aquarium Zoo New York Zoo Zoo Zoo Zoo']' Scores ='tensor([[ -86.3080,  -86.0185,  -97.5714,  ..., -105.4071,  -97.0823,\n",
            "          -84.9393]], device='cuda:0')' \n"
          ]
        }
      ]
    }
  ]
}
