{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMezFbFrTeo90RjB7EQBRta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/tranformer_learn/blob/main/LLM_Loss_Understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "T4KJz5eHPGon"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kkRuIWvHXUP0"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our small Training and Target data"
      ],
      "metadata": {
        "id": "h_koippHelbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text  = \"Welcome to New York Zoo\" # As New York City is most common and the pre-trained model may predict it\n",
        "target_text = input_text"
      ],
      "metadata": {
        "id": "pojXjA00epF7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model, Tokenizer etc"
      ],
      "metadata": {
        "id": "2yPUjMp3dDUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from Karpathy and modified\n",
        "# https://github.com/karpathy/nanoGPT/blob/086ebe1822791b775e951b4b562fbb7131d83cc2/train.py\n",
        "def get_batch(len_train_data,input_ids,attention_mask,device,block_size,\n",
        "                    batch_size):\n",
        "    #print(f\"len_train_data={len_train_data} block_size ={block_size} batch_size= {batch_size}\")\n",
        "\n",
        "    if len_train_data > block_size:\n",
        "      ix = torch.randint(0,len_train_data-block_size , (batch_size,)) # random select from training data set\n",
        "    else:\n",
        "     ix = torch.zeros(batch_size, dtype=torch.int) # else give all data as is but in batches\n",
        "    #print(f\"ix {ix.shape} ={ix.tolist()}\")\n",
        "    x = torch.stack([(input_ids[i:i+block_size]) for i in ix])\n",
        "    y = torch.stack([((attention_mask[i:i+block_size])) for i in ix])\n",
        "    #print(x.shape,x)\n",
        "    # # here is the encoding\n",
        "    # torch.Size([2, 5]) tensor([[14618,   284,   968,  1971, 21980],\n",
        "    #    [14618,   284,   968,  1971, 21980]])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "bQFHQW8IjBnT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import shutil\n",
        "from transformers import  get_linear_schedule_with_warmup # for training\n",
        "from datetime import datetime\n",
        "import re\n",
        "import torch._dynamo.config\n",
        "\n",
        "\n",
        "model_name = 'gpt2'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "id = tokenizer.encode('[PAD]')\n",
        "tokenizer.pad_token = id\n",
        "print(id)\n",
        "#print(TokenizerDetails(tokenizer) # model_max_length: 1024 # vocab_size: 50257\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)#,pad_token_id=id)\n",
        "#model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "-V17qEasX_we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7affdf-4a93-449e-afa1-87a1d78ed4fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USE CPU if GPU is not available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ClUPMJydwWM",
        "outputId": "a4229834-37b4-4a81-a7b1-b32112164da3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval Model before Training"
      ],
      "metadata": {
        "id": "7AIsT98sdOj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# encode the inputs\n",
        "encoding = tokenizer(input_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "# encode the targets\n",
        "target_encoding = tokenizer(target_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "labels = target_encoding.input_ids\n",
        "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "labels[labels == tokenizer.pad_token_id] = -100  # in our case there is no padding\n",
        "print(f\"input_ids={input_ids}\")\n",
        "print(f\"attention_mask={attention_mask}\") # all ones\n",
        "print(f\"labels ={labels}\")\n",
        "# forward pass\n",
        "outputs = model(input_ids=input_ids.to(device),labels=labels.to(device))\n",
        "print(f\"Model Loss Before training for the Target {outputs.loss}\")\n",
        "# Test the model to check what it predicts next\n",
        "# remove the last token off for input-id's as well as attention Mask\n",
        "input_ids = input_ids[:,:-1] # input_text  = \"Welcome to New York\"\n",
        "attention_mask = attention_mask[:,:-1]\n",
        "print(f\"input_ids={input_ids}\")\n",
        "outputs = model.generate(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device),max_new_tokens=1)\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(f\"Result '{answer}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTIWd6szXVu7",
        "outputId": "09daf476-c225-49fe-a250-bac92e034ed2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids=tensor([[14618,   284,   968,  1971, 21980]])\n",
            "attention_mask=tensor([[1, 1, 1, 1, 1]])\n",
            "labels =tensor([[14618,   284,   968,  1971, 21980]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loss Before training for the Target 4.441195011138916\n",
            "input_ids=tensor([[14618,   284,   968,  1971]])\n",
            "Result 'Welcome to New York City'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# we are using Greedy Search by default - see https://huggingface.co/blog/how-to-generate\n",
        "# try explicit\n",
        "# outputs = model.greedy_search(input_ids=input_ids.to(device),max_new_tokens=1,output_scores=True, return_dict_in_generate=True,pad_token_id=50257) #out of memory\n",
        "outputs = model.generate(input_ids=input_ids.to(device),max_new_tokens=1,output_scores=True, return_dict_in_generate=True)\n",
        "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "#answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    score = score.cpu()\n",
        "    print(f\"| token | token string | logits | probability\")\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
        "\n",
        "print(f\"Possible ouputs '{tokenizer.batch_decode(outputs.sequences,skip_special_tokens=False)}' Scores ='{outputs.scores[0]}' \")\n",
        "# see also what this loss actually mean https://stackoverflow.com/a/75712209/429476"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAgwJYTruRO-",
        "outputId": "7d7466b8-7f2c-45ac-9b50-f8b2683db1bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| token | token string | logits | probability\n",
            "|  2254 |  City    | -1.261 | 28.35%\n",
            "Possible ouputs '['Welcome to New York City']' Scores ='tensor([[-66.2701, -69.3933, -74.2934,  ..., -79.2006, -79.7269, -68.8056]],\n",
            "       device='cuda:0')' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "pZELm4Y0c2YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train() # set model for training\n",
        "\n",
        "\n",
        "print(f\"length of dataset in words: {len(input_text):,}\") #252,023\n",
        "\n",
        "encoding = tokenizer(input_text, truncation=True, padding=True,return_tensors='pt')\n",
        "print(f\"encoding.input_ids.shape {encoding.input_ids.shape}\")\n",
        "#encoding.input_ids.shape torch.Size([1, 6])\n",
        "\n",
        "print(f\"encoding.attention_mask.shape {encoding.attention_mask.shape}\")\n",
        "len_train_data = encoding.input_ids.shape[1]\n",
        "print(f\"len_train_data = {len_train_data}\")\n",
        "# len_train_data = 6\n",
        " # flatten the tensor from  torch.Size([1, 6]) to  torch.Size([48735])\n",
        "input_ids=encoding.input_ids.view(-1)\n",
        "attention_mask=encoding.attention_mask.view(-1)\n",
        "# Note , if we give truncation as False then the token sequence length goes more than model_max_length\n",
        "# Token indices sequence length is longer than the specified maximum sequence length for this\n",
        "#  model (23552 > 1024). Running this sequence through the model will result in indexing errors\n",
        "# However we are not running through the model; We will add it to an array and train with block_size\n",
        "\n",
        "# Load the  model\n",
        "\n",
        "# # Freeze bottom 10 layers\n",
        "# for parameter in model.parameters():\n",
        "#     parameter.requires_grad = False\n",
        "\n",
        "for i, m in enumerate(model.transformer.h):\n",
        "    #Only un-freeze the last n transformer blocks\n",
        "    if i >= 10:\n",
        "        for parameter in m.parameters():\n",
        "            parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.transformer.ln_f.parameters():\n",
        "    parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.lm_head.parameters():\n",
        "    parameter.requires_grad = True\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "# learning_rate = 6e-4 # ??\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
        "\n",
        "# Set up the training parameters\n",
        "train_batch_size = 1\n",
        "print(f\"len_train_data = {len_train_data}\")\n",
        "block_size = len_train_data +1\n",
        "num_train_epochs = 50\n",
        "\n",
        "# Set the optimizer and learning rate scheduler\n",
        "# num_warmup_steps = 100\n",
        "# max_grad_norm = 1.0\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "num_train_steps = len_train_data // train_batch_size * num_train_epochs\n",
        "#lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    #print(f\"Epoch {epoch+1} of {num_train_epochs}\")\n",
        "    epoch_loss = 0\n",
        "    for i in range(0,len_train_data, block_size):\n",
        "        # do the batch size manipulation here\n",
        "        x,y= get_batch(len_train_data,input_ids,attention_mask,device,\n",
        "            block_size=block_size,batch_size=train_batch_size)\n",
        "        # attention_mask given by tokenize is array of ones= [1,1,..], that is attend to all tokens\n",
        "        # if we do not give the parameter, the model will attend to all tokens by default\n",
        "        outputs = model(input_ids=x,attention_mask=y,labels=x)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        #lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch} complete. Loss: {loss.item()} \")\n",
        "\n",
        "print(f\"Epoch {epoch} complete. Loss: {loss.item()} \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2v_sQvWc08D",
        "outputId": "e5ab16f7-5e90-40ac-dd5d-747a1e651a21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in words: 23\n",
            "encoding.input_ids.shape torch.Size([1, 5])\n",
            "encoding.attention_mask.shape torch.Size([1, 5])\n",
            "len_train_data = 5\n",
            "len_train_data = 5\n",
            "Epoch 0 complete. Loss: 5.066590309143066 \n",
            "Epoch 1 complete. Loss: 3.318091869354248 \n",
            "Epoch 2 complete. Loss: 2.5140624046325684 \n",
            "Epoch 3 complete. Loss: 2.901312828063965 \n",
            "Epoch 4 complete. Loss: 1.245482325553894 \n",
            "Epoch 5 complete. Loss: 0.9293510317802429 \n",
            "Epoch 6 complete. Loss: 0.7387741804122925 \n",
            "Epoch 7 complete. Loss: 1.73149573802948 \n",
            "Epoch 8 complete. Loss: 0.451931893825531 \n",
            "Epoch 9 complete. Loss: 0.1960199475288391 \n",
            "Epoch 10 complete. Loss: 0.11847665160894394 \n",
            "Epoch 11 complete. Loss: 0.09804023057222366 \n",
            "Epoch 12 complete. Loss: 0.07704108953475952 \n",
            "Epoch 13 complete. Loss: 0.013933488167822361 \n",
            "Epoch 14 complete. Loss: 0.00468256464228034 \n",
            "Epoch 15 complete. Loss: 0.004392709583044052 \n",
            "Epoch 16 complete. Loss: 0.0023247734643518925 \n",
            "Epoch 17 complete. Loss: 0.001137734274379909 \n",
            "Epoch 18 complete. Loss: 0.000593673437833786 \n",
            "Epoch 19 complete. Loss: 1.0290896892547607 \n",
            "Epoch 20 complete. Loss: 0.00015851993521209806 \n",
            "Epoch 21 complete. Loss: 0.9821516275405884 \n",
            "Epoch 22 complete. Loss: 0.00019329521455802023 \n",
            "Epoch 23 complete. Loss: 0.00017373438458889723 \n",
            "Epoch 24 complete. Loss: 0.0001120952219935134 \n",
            "Epoch 25 complete. Loss: 0.00031261451658792794 \n",
            "Epoch 26 complete. Loss: 5.5636275646975264e-05 \n",
            "Epoch 27 complete. Loss: 0.9873984456062317 \n",
            "Epoch 28 complete. Loss: 0.00022562945378012955 \n",
            "Epoch 29 complete. Loss: 0.9719550013542175 \n",
            "Epoch 30 complete. Loss: 0.982638418674469 \n",
            "Epoch 31 complete. Loss: 1.9967159460065886e-05 \n",
            "Epoch 32 complete. Loss: 2.178500835725572e-05 \n",
            "Epoch 33 complete. Loss: 6.776637019356713e-05 \n",
            "Epoch 34 complete. Loss: 5.212193354964256e-05 \n",
            "Epoch 35 complete. Loss: 0.00010026992822531611 \n",
            "Epoch 36 complete. Loss: 0.00011164928582729772 \n",
            "Epoch 37 complete. Loss: 0.9640589356422424 \n",
            "Epoch 38 complete. Loss: 0.8493282198905945 \n",
            "Epoch 39 complete. Loss: 1.0013482096837834e-05 \n",
            "Epoch 40 complete. Loss: 5.7214423577534035e-05 \n",
            "Epoch 41 complete. Loss: 0.0289620291441679 \n",
            "Epoch 42 complete. Loss: 5.149445132701658e-05 \n",
            "Epoch 43 complete. Loss: 2.139754178642761e-05 \n",
            "Epoch 44 complete. Loss: 0.000903725391253829 \n",
            "Epoch 45 complete. Loss: 0.9292662143707275 \n",
            "Epoch 46 complete. Loss: 3.886078775394708e-05 \n",
            "Epoch 47 complete. Loss: 1.0579731679172255e-05 \n",
            "Epoch 48 complete. Loss: 0.9201220273971558 \n",
            "Epoch 49 complete. Loss: 0.9379816055297852 \n",
            "Epoch 49 complete. Loss: 0.9379816055297852 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval after Training"
      ],
      "metadata": {
        "id": "4953RZGLc6tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# encode the inputs\n",
        "encoding = tokenizer(input_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "# encode the targets\n",
        "target_encoding = tokenizer(target_text,padding=True,truncation=True,return_tensors=\"pt\",)\n",
        "labels = target_encoding.input_ids\n",
        "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "labels[labels == tokenizer.pad_token_id] = -100  # in our case there is no padding\n",
        "print(f\"input_ids={input_ids}\")\n",
        "print(f\"attention_mask={attention_mask}\") # all ones\n",
        "print(f\"labels ={labels}\")\n",
        "# forward pass\n",
        "outputs = model(input_ids=input_ids.to(device),labels=labels.to(device))\n",
        "print(f\"Model Loss After  training for the Target {outputs.loss}\")\n",
        "# Test the model to check what it predicts next\n",
        "# remove the last token off for input-id's as well as attention Mask\n",
        "input_ids = input_ids[:,:-1]\n",
        "attention_mask = attention_mask[:,:-1]\n",
        "print(f\"input_ids={input_ids}\")\n",
        "outputs = model.generate(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device),max_new_tokens=1)\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(f\"Result '{answer}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cki96OWZP7-",
        "outputId": "5b574cb7-34aa-4aae-8943-964b4660139a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids=tensor([[14618,   284,   968,  1971, 21980]])\n",
            "attention_mask=tensor([[1, 1, 1, 1, 1]])\n",
            "labels =tensor([[14618,   284,   968,  1971, 21980]])\n",
            "Model Loss After  training for the Target 0.006892044097185135\n",
            "input_ids=tensor([[14618,   284,   968,  1971]])\n",
            "Result 'Welcome to New York Zoo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids=input_ids.to(device),max_new_tokens=1,output_scores=True, return_dict_in_generate=True)\n",
        "# https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075\n",
        "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "#answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    score = score.cpu()\n",
        "    print(f\"| token | token string | logits | probability\")\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
        "\n",
        "print(f\"Possible ouputs '{tokenizer.batch_decode(outputs.sequences,skip_special_tokens=False)}' Scores ='{outputs.scores[0]}' \")\n",
        "# see also https://stackoverflow.com/a/75712209/429476"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaLck0RLHpYr",
        "outputId": "0c31be09-98fc-4aa0-8425-93322d8ba837"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.000 | 100.00%\n",
            "Possible ouputs '['Welcome to New York Zoo']' Scores ='tensor([[-61.1597, -60.4100, -70.5719,  ..., -81.2571, -72.4281, -60.9752]],\n",
            "       device='cuda:0')' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs = model.generate(input_ids=input_ids.to(device),max_new_tokens=1,num_return_sequences=4,num_beams=4,output_scores=True, return_dict_in_generate=True)\n",
        "outputs = model.generate(input_ids=input_ids.to(device),max_new_tokens=12,output_scores=True, return_dict_in_generate=True)\n",
        "# https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075\n",
        "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "#answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "input_length =  input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    score = score.cpu()\n",
        "    print(f\"| token | token string | logits | probability\")\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
        "\n",
        "#print(f\"Possible ouputs '{outputs.sequences}' Scores ='{outputs.scores}' \")\n",
        "print(f\"Possible ouputs '{tokenizer.batch_decode(outputs.sequences,skip_special_tokens=False)}' Scores ='{outputs.scores[0]}' \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQNItmmPXqn5",
        "outputId": "d663bdb0-9984-440b-a357-86479d152347"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | 0.000 | 100.00%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.649 | 52.28%\n",
            "| token | token string | logits | probability\n",
            "|    11 | ,        | -1.212 | 29.77%\n",
            "| token | token string | logits | probability\n",
            "|   968 |  New     | -0.019 | 98.15%\n",
            "| token | token string | logits | probability\n",
            "|  1971 |  York    | -0.000 | 100.00%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.001 | 99.86%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.517 | 59.64%\n",
            "| token | token string | logits | probability\n",
            "|    11 | ,        | -1.137 | 32.07%\n",
            "| token | token string | logits | probability\n",
            "|   968 |  New     | -0.106 | 89.94%\n",
            "| token | token string | logits | probability\n",
            "|  1971 |  York    | -0.000 | 99.98%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.001 | 99.86%\n",
            "| token | token string | logits | probability\n",
            "| 21980 |  Zoo     | -0.081 | 92.21%\n",
            "Possible ouputs '['Welcome to New York Zoo Zoo, New York Zoo Zoo, New York Zoo Zoo']' Scores ='tensor([[-61.1597, -60.4100, -70.5719,  ..., -81.2571, -72.4281, -60.9752]],\n",
            "       device='cuda:0')' \n"
          ]
        }
      ]
    }
  ]
}