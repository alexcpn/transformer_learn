# Generated by ChatGPT + Updated 

from transformers import GPT2Tokenizer, GPT2Model
import torch
from utils import printTokenizerDetails
# Initialize the GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
#printTokenizerDetails(tokenizer) # model_max_length: 1024 # vocab_size: 50257

# Define the input text
input_text = "This is a long input text sequence that we want to encode with the GPT-2 tokenizer and model."
for i in range(0,10):
    input_text += input_text 

encoding = tokenizer(input_text, truncation=False, padding=True,return_tensors='pt')
print(len(input_text))
print(encoding.input_ids.shape)
print(encoding.attention_mask.shape)

# Note , if we give truncation as False then the token sequence length goes more than model_max_length
# Token indices sequence length is longer than the specified maximum sequence length for this
#  model (23552 > 1024). Running this sequence through the model will result in indexing errors
# 95232
# torch.Size([1, 23552])
# torch.Size([1, 23552])

# However we are not running through the model; We will add it to an array and train with block_size

block_size = tokenizer.model_max_length # 1024

