# -*- coding: utf-8 -*-
"""llama2-7b-4bit-qa_generator_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lZdGovQbEKwEmKzSWfx1JMnmdPNphgjz
"""

!pip install -q transformers[sentencepiece] torch  langchain sentence_transformers faiss-gpu accelerate peft huggingface_hub bitsandbytes

import torch
import traceback
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
   AutoConfig,

)
from accelerate import infer_auto_device_map ,init_empty_weights

class LLAMa2Q:
    def __init__(self,model_name,q4bitA=True) -> None:

        if  not torch.cuda.is_available() :
            print("Running 4 bit quantised model is possible only on GPU")
            raise Exception("Cannot Proceed without GPU")

        ################################################################################
        # bitsandbytes parameters
        ################################################################################

        # Activate 4-bit precision base model loading
        use_4bit = True

        # Compute dtype for 4-bit base models
        bnb_4bit_compute_dtype = "float16"

        # Quantization type (fp4 or nf4)
        bnb_4bit_quant_type = "nf4"
        # Quantization type (fp4 or nf4)
        bnb_8bit_quant_type = "nf8"

        # Activate nested quantization for 4-bit base models (double quantization)
        use_nested_quant = False

        # Load the entire model on the GPU 0
        device_map = {"": 0}
        self.device = torch.device("cuda")


        model_name =model_name

        # Load tokenizer and model with QLoRA configuration
        compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

        if q4bitA:
          bnb_config = BitsAndBytesConfig(
              load_in_4bit=True,
              bnb_4bit_quant_type=bnb_4bit_quant_type,
              bnb_4bit_compute_dtype=compute_dtype,
              bnb_4bit_use_double_quant=use_nested_quant,
          )
        else:
          # Load in 8 bit
          bnb_config = BitsAndBytesConfig(
              load_in_8bit=True,
              bnb_8bit_quant_type=bnb_8bit_quant_type,
              bnb_8bit_compute_dtype=compute_dtype,
              bnb_8bit_use_double_quant=use_nested_quant,
              llm_int8_enable_fp32_cpu_offload=True,
        )

        # # # Load in 8 bit
        # self.model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config,torch_dtype=torch.bfloat16,
        #                                             device_map=device_map)

        # # Load in 4 bit
        self.model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config,
                                                    device_map=device_map)

        #torch.save(self.model.state_dict(), "4bit")
        # Load LLaMA tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print('model_max_length:', self.tokenizer.model_max_length)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        # self.tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

    def create_prompt(self,question,system_message):
        prompt = f"{ question}"
        if not system_message:
            system_message = "You are a helpful assistant.Please answer the question if it is possible"
        prompt_template=f'''[INST] <<SYS>>
        {system_message}
        <</SYS>>

        {prompt} [/INST]'''
        #print(prompt_template)
        return prompt_template

    def generate_ouputput(self,prompt_template):
        inputs = self.tokenizer(prompt_template, return_tensors="pt",truncation=True,max_length=2000)
        outputs = self.model.generate(**inputs.to(self.device) ,max_new_tokens=2000)
        output = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        return output

    def get_chat_response(self,output):
        outputcont = "".join(output)
        parts = outputcont.split("[/INST]", 1)
        return parts[1]

!huggingface-cli login

print("Going to load the llama2 7b model ...")
llam2_4bit = LLAMa2Q(model_name="meta-llama/Llama-2-7b-chat-hf",q4bitA=True)
print("Loaded LLama2 7b model")

#upload files to your colab environment
!wget https://gist.githubusercontent.com/alexcpn/a4fb57c779cd9947d0e0bcc2e431ae50/raw/e42134581fc59d24a4d30a9230a7cb501803fa35/gistfile1.txt

import re
import locale
locale.getpreferredencoding = lambda: "UTF-8"

pattern = r"(?:Q:|Question:)\s*(.*?)\n(?:A:|Answer:)\s*(.*?)(?=\n(?:Q:|Question:)|\n$)"

def parse_response_out (response):
  return  re.findall(pattern, response, re.DOTALL)

def sliding_window(filename, window_size=200, overlap=50):
    with open(filename, 'r', encoding='utf-8') as file:
        text = file.read()
        words = text.split()

        step = window_size - overlap
        chunks = []
        for i in range(0, len(words) - window_size + 1, step):
            chunk = ' '.join(words[i:i + window_size])
            chunk = remove_broken_sentences(chunk)
            chunks.append(chunk)

        return chunks

def remove_broken_sentences(chunk):
    # Split the chunk into sentences
    sentences = re.split(r'(?<=[.!?])\s+', chunk)

    # Check and remove the first sentence if it doesn't start with a capital letter
    if not sentences[0][0].isupper():
        sentences = sentences[1:]

    # Check and remove the last sentence if it doesn't start with a capital letter
    if not sentences[-1][0].isupper():
        sentences = sentences[:-1]

    return ' '.join(sentences)

system_message = """You are a smart AI assistant that can create question and answers based on the context"""

filename = 'gistfile1.txt'
chunks = sliding_window(filename)

for index, chunk in enumerate(chunks):
    promt =f"""context = '{chunk}'
     Create 20 different questions and answers for the above context, format like
     Q:
     A:
     """
    prompt_template = llam2_4bit.create_prompt(promt,system_message)
    try:
        output = llam2_4bit.generate_ouputput(prompt_template)
        response =llam2_4bit.get_chat_response(output)
        print(response)
        matches = parse_response_out(response)
        with open("qa_data2.txt", "a", encoding='utf-8') as myfile:
          for index, (question, answer) in enumerate(matches):
            out = f"<s>[INST] Source:8989REF {question} [/INST] Source:8989REF {answer} </s>\n"
            myfile.write(out)
        !cp qa_data2.txt ./drive/MyDrive/models
    except Exception :
        print(traceback.format_exc())

!head qa_data2.txt
!cp qa_data2.txt ./drive/MyDrive/models

print("Going to load the llama2 13b model ...")
llam2_4bit = LLAMa2Q(model_name="meta-llama/Llama-2-13b-chat-hf",q4bitA=True)
print("Loaded LLama2 7b model")

system_message = """You are a smart AI assistant that can create question and answers based on the context"""
promt =f"""What is the relation between Igodo and Ranrak?
  """
prompt_template = llam2_4bit.create_prompt(promt,system_message)
output = llam2_4bit.generate_ouputput(prompt_template)
response =llam2_4bit.get_chat_response(output)
print(response)