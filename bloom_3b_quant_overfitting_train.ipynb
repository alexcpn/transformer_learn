{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16VQg4EuMdoYxMzbU85L-S_DPTxbtCTFc",
      "authorship_tag": "ABX9TyNIVIelnA4u7dJDBDXcAf5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/tranformer_learn/blob/main/bloom_3b_quant_overfitting_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNSFcDPBBXy7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n",
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT - Parameter Effecient Training\n",
        "\n",
        "LoRA - Low Randk Adapter (one techinque of PEFT)\n",
        "\n",
        "https://huggingface.co/blog/peft\n"
      ],
      "metadata": {
        "id": "U7MBm_ooAoJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "import torch\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "torch.ones((1, 1)).to(\"cuda\")\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJLMBfs4q4Qq",
        "outputId": "c6cb33a4-ae46-4847-e903-d9fc390987d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 363 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upload files to your colab environment\n",
        "!wget https://raw.githubusercontent.com/alexcpn/tranformer_learn/main/data/small_3.txt\n",
        "#!wget https://gist.githubusercontent.com/alexcpn/54e88130f9d186494f1c3ce5e83263b4/raw/7cdf5f93b819024c58a891fc808fbdbe052d0eb1/small_3_mixed.txt\n",
        "train_path = 'small_3.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLV1aqxZDI8O",
        "outputId": "e3b752b9-e72d-4516-8741-6052659b6171"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-27 13:12:21--  https://raw.githubusercontent.com/alexcpn/tranformer_learn/main/data/small_3.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56513 (55K) [text/plain]\n",
            "Saving to: â€˜small_3.txt.1â€™\n",
            "\n",
            "small_3.txt.1       100%[===================>]  55.19K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-06-27 13:12:22 (7.14 MB/s) - â€˜small_3.txt.1â€™ saved [56513/56513]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def load_dataset(path,tokenizer):\n",
        "    dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return dataset,data_collator\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "train_dataset,data_collator = load_dataset(train_path,tokenizer)\n",
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1-dtEE0Ecbp",
        "outputId": "4491025e-1b8d-458f-e762-8870a70b2220"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 363 MB.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "from peft import LoraConfig, PeftModel, PeftConfig, get_peft_model,TaskType\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# lora_config = {\n",
        "#     \"r\": 16,# attention heads\n",
        "#     \"lora_alpha\": 32, # alpha scaling\n",
        "#     \"lora_dropout\": 0.05,\n",
        "#     'bias': \"none\",\n",
        "#     \"task_type\": \"CAUSAL_LM\", # set this for CLM or Seq2Seq\n",
        "\n",
        "# }\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
        ")\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bigscience/bloom-3b\", device_map='auto',load_in_8bit=True)\n",
        "#model = get_peft_model(model, LoraConfig(**lora_config))\n",
        "model = get_peft_model(model, peft_config)\n",
        "#print(f\"Model trainable parameters:\\n {print_trainable_parameters(model)}\")\n",
        "\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d6QgbMrG4yD",
        "outputId": "506caa95-1fde-4a70-f100-941f9c99ba3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 11179 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bloom-3b-small3-v1\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=100, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=1000, # after # steps model is saved\n",
        "    save_total_limit=2,\n",
        "    warmup_steps=200,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    fp16= True,\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    #eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "_9lH0vBIsOIE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "psbZA3YMHF2z",
        "outputId": "79922409-fe79-4216-c5f9-e5df9bd54b68"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2400' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2400/2400 56:35, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.124200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.545300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.139700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.061100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2400, training_loss=0.6040298612912496, metrics={'train_runtime': 3398.723, 'train_samples_per_second': 2.736, 'train_steps_per_second': 0.706, 'total_flos': 1.6875793022976e+16, 'train_loss': 0.6040298612912496, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "aVehIp-JHwsO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.to_json_file(\"./bloom-3b-small3-v1/config.json\")"
      ],
      "metadata": {
        "id": "YzEtJVE41kSz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r bloom-3b-small3-v1-lora2.zip bloom-3b-small3-v1/config.json  bloom-3b-small3-v1/training_args.bin  bloom-3b-small3-v1/adapter_model.bin bloom-3b-small3-v1/adapter_config.json\n",
        " !cp bloom-3b-small3-v1-lora2.zip ./drive/MyDrive/models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PttMdSZY_9_v",
        "outputId": "ed428ca5-b7d8-48e1-a305-28c2520bac17"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: bloom-3b-small3-v1/config.json (deflated 53%)\n",
            "  adding: bloom-3b-small3-v1/training_args.bin (deflated 49%)\n",
            "  adding: bloom-3b-small3-v1/adapter_model.bin (deflated 7%)\n",
            "  adding: bloom-3b-small3-v1/adapter_config.json (deflated 37%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'bloom-3b-small3-v1-modelstate.zip')"
      ],
      "metadata": {
        "id": "A4Xec82EAmgr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !cp bloom-3b-small3-v1-modelstate.zip ./drive/MyDrive/models"
      ],
      "metadata": {
        "id": "TXJdKRNTRH35"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "XaH_ZIfUmGAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "pretrained = \"./bloom-3b-small3-v1\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(pretrained)\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bigscience/bloom-3b\", device_map='auto',load_in_8bit=True)\n",
        "model = PeftModel.from_pretrained(model, pretrained)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CCBvDyBmc7",
        "outputId": "c6bc8863-0fc7-40f6-8443-495603b7361e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1362: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): BloomForCausalLM(\n",
              "      (transformer): BloomModel(\n",
              "        (word_embeddings): Embedding(250880, 2560)\n",
              "        (word_embeddings_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (h): ModuleList(\n",
              "          (0-29): 30 x BloomBlock(\n",
              "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (self_attention): BloomAttention(\n",
              "              (query_key_value): Linear8bitLt(\n",
              "                in_features=2560, out_features=7680, bias=True\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=7680, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): BloomMLP(\n",
              "              (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
              "              (gelu_impl): BloomGelu()\n",
              "              (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2560, out_features=250880, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "#test = pipeline('text-generation',model='./bloom-3b-small3-v1/', tokenizer='bigscience/bloom-3b')\n",
        "test = pipeline('text-generation',model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "XN-eoDuiHXPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64b253c-6b8f-4248-82cf-9ee8b0f8f553"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  prompt = \"what is bacteria\"\n",
        "  encoded_input = tokenizer(prompt,truncation=True,padding=True, return_tensors='pt')\n",
        "  test_output_2 = model.generate(input_ids=encoded_input.input_ids,\n",
        "                  max_new_tokens=100,\n",
        "                  num_return_sequences=1,\n",
        "                  early_stopping=True)\n",
        "  test_answer_2 = tokenizer.decode(test_output_2[0], skip_special_tokens=True)\n",
        "  print(f\"Generated test_answer_1 : {test_answer_2}\")\n"
      ],
      "metadata": {
        "id": "CKyPR4inIg9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba6c9e5-fcd0-4326-b358-a1d20b7e0fb2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated test_answer_1 : what is bacteria called a  spore  is the material by which a  bacillus  is produced. When a  bacillus  is exposed to air and has been freed from its water of deposit, such as its  spore  is said to have  survived of the body , and is designated by the present name of the book. sp. a. The  sp. a. of a  bacillus  is necessary for its multiplication. If the  sp. a. of a  bac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out = test('Streptococci are met with in', max_new_tokens=120,num_return_sequences=1)\n",
        "  print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5fhnJ8XFQ3Y",
        "outputId": "f8210253-6a22-4034-c1dc-ae02297a38be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Streptococci are met with in great abundance, and of those of the Streptococcus Actinomycin  type  bacilli  in particular districts; and of those of the Actinomycin  type  bacteria, in general areas. In the young and active tissues, the bacilli produce only feebly active disease, and are almost universally met with in the tissues of the bones and of the head. In more than phÃ¢n of the tissues in which Actinomycin  bacteria are found  act, or actin, is produced. Act is formed in the organisms of the a streptomycin group by certain of its'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out =test('Streptococci', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsN0r9eeYUgJ",
        "outputId": "c3898d60-ecc7-4a50-ae5d-bc0dcd097843"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Streptococci, Streptococcic Acids, Acido-Streptococcic,  Streptococcic, as they are known. The term  Streptococcus  is now only rarely used in pathology, and is only applied to organisms resembling those of the group mentioned at page 141. Those most frequently employed  are,  Streptococcus Aureus, or  Streptococcus Anaerobic, as it is called by its unique feature of not being capable of being developed'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out =test('Metchnikoff', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdvxhuI2q3Ki",
        "outputId": "d25ef545-533a-4edd-cb46-cd1c09755e00"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Metchnikoff's method. In this method the bacteria are suspended in water and injected into the peritoneal cavity. After having been introduced for from one to two days, when they appear to have been killed, the bacteria are assumed to be metabolically inert, and are taken up by certain portions of the body cells, in particular the cells of the lumen of the lungs and the cells of the peritoneal cavity. These cells metabolise the bacteria which have been introduced, and have metastasized,\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out =test('To this process Metchnikoff', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQnX1haEq28P",
        "outputId": "2c7df893-8417-4ef3-cc59-06b39481dfb3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'To this process Metchnikoff has given the name  vernix caseosa  or  dermato-quinum, from the fact that it is rendered hyaline after being placed in alcohol. The method of applying the compound vaccine is now being abandoned, and the surgeon is being asked to render this method as safe as possible. He first scrapes up a certain amount of skin from the site of the grafting, makes sure that it is free from bacteria, and applies aseptic gauze to prevent the introduction of'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out = test('phagocytosis', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89o6DTDRHnZ4",
        "outputId": "f088f47c-c5ea-4770-8ea9-491b8e37028a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'phagocytosis,  phagocytosis, the process by which a foreign substance is taken up and degraded by the cells of the mononuclear and other inflammatory tissues. The most important effect of this action is the removal of the substance from sight, the cells of the target tissue being covered with a coat of black phagophore, produced by the great, vital, organisms. The action is further promoted by the fact that the cells of the target tissue attempt to phagocyte the new organisms and'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out =test('During the process of phagocytosis,', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ls3NLVH8FK",
        "outputId": "abecb71d-ff24-4fca-ebf7-9c4a249db7a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'During the process of phagocytosis, the protoplasm of certain organisms is ingested with the bacteria, and during the process of decomposition the granulation tissue formed around the various organisms forms the base of the firm tissue reaction. In the early stages of the disease of infancy  bacterial peritonitis  may follow and, although its cause may eventually be determined, the swelling and the reaction of the peritoneum may at times reveal the name of the hospital environment in which the organism was originally stored.  Peritoneum-Fixed'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out =test(' diplococci ', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWXYJWCtmdIA",
        "outputId": "e2c3ba19-e120-4f8c-846c-fef7d359b642"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': ' diplococci  a good source of a product called  acido-philic protoplasm  the bacteria may also derive their energy  fermentative  fermentation is the term used to describe the process by which protoplasm containing carbon dioxide is produced in the presence of certain bacteria. The  energy  of fermentation is derived from the  nourishing phosphorus  present in the food. The  bacteria  which are employed in deriving their energy are known as  aÃ©ro-philes  or  aÃ©ro-biles.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out = test('Cocci  or  micrococci', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwWTYGjzoQfF",
        "outputId": "6cc98a14-bd4c-47f9-a362-fa4874f65c49"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Cocci  or  micrococci. In the treatment of open wounds, the great majority of organisms play the role of phagocytes, and the most important of all are the monocytes and the micrococci. A few cells ofçƒ­ä¼‘å…‹ç»†èƒž (thermic shock cells) also play a part, but their function is to protect the work of the phagocytes by blocking the surface of the wound and preventing the excessive accumulation of red blood cells. The organism which is selected for by the phagocytes may be a'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out =test('Bacteria are most conveniently', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JszU6_YksrTI",
        "outputId": "05ef6764-8264-4a57-c17b-8911479ba0c5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Bacteria are most conveniently preserved in the  cold air, as the pressure of which the water of amnesia is capable of, the temperature at which it is given and the duration of time that are factors in determining whether it is that of the environment in which the bacteria have been called that it may live and produce bacteriology, or they be taken in and developed in the body itself. The air should be kept at about  60 in. and the bacteria are most effectively preserved if the observer and the observer's\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out = test('given the context \"Thus we recognise (1) those that are globular  cocci ; (2) those that resemble a rod  bacilli ; (3) the spiral or wavy forms  spirilla .  Cocci  or  micrococci  are minute round bodies, averaging about 1 Âµ in diameter. The great majority are non-motile. They multiply by fission; and when they divide in such a way that the resulting cells remain in pairs, are called  diplococci , of which the bacteria of gonorrhÅ“a and pneumonia are examples (Fig. 5). When they divide irregularly, and form grape-like bunches, they are known as  staphylococci , and to this variety the commonest pyogenic or pus-forming organisms belong' +\n",
        "  'answer \"What are Cocci  or  micrococci', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ujnSq4EZ0_l",
        "outputId": "de2c51e6-fbc9-4a72-b7d0-f42936ea7517"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'given the context \"Thus we recognise (1) those that are globular  cocci ; (2) those that resemble a rod  bacilli ; (3) the spiral or wavy forms  spirilla .  Cocci  or  micrococci  are minute round bodies, averaging about 1 Âµ in diameter. The great majority are non-motile. They multiply by fission; and when they divide in such a way that the resulting cells remain in pairs, are called  diplococci , of which the bacteria of gonorrhÅ“a and pneumonia are examples (Fig. 5). When they divide irregularly, and form grape-like bunches, they are known as  staphylococci , and to this variety the commonest pyogenic or pus-forming organisms belonganswer \"What are Cocci  or  micrococci?\" is one of the questions in this chapter (page 21).  Bacilli  are long, slender bodies, with a double membrane dividing them into pairs or bunches. The most familiar examples are those derived from animals such as the bacteria of the human and animal pyogenic diseases (Fig. 6). When the double membrane is lost and the cells are simply elongated and fusiform, they are known as  bacilli  or  bacteria. These cells multiply by fission; and, when this'}]\n"
          ]
        }
      ]
    }
  ]
}