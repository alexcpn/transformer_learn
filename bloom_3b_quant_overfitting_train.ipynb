{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16VQg4EuMdoYxMzbU85L-S_DPTxbtCTFc",
      "authorship_tag": "ABX9TyO20i0sjkbwHvN+b+7Sh9sV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/tranformer_learn/blob/main/bloom_3b_quant_overfitting_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNSFcDPBBXy7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.28.1\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n",
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "import torch\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "torch.ones((1, 1)).to(\"cuda\")\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJLMBfs4q4Qq",
        "outputId": "38dc1e23-0397-4472-d2ff-866254b4c7b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 363 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upload files to your colab environment\n",
        "!wget https://raw.githubusercontent.com/alexcpn/tranformer_learn/main/data/small_3.txt\n",
        "#!wget https://gist.githubusercontent.com/alexcpn/54e88130f9d186494f1c3ce5e83263b4/raw/7cdf5f93b819024c58a891fc808fbdbe052d0eb1/small_3_mixed.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLV1aqxZDI8O",
        "outputId": "54874f8e-c237-4c10-bc44-07ee3d0801c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-20 07:20:10--  https://raw.githubusercontent.com/alexcpn/tranformer_learn/main/data/small_3.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56513 (55K) [text/plain]\n",
            "Saving to: â€˜small_3.txt.2â€™\n",
            "\n",
            "small_3.txt.2       100%[===================>]  55.19K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-06-20 07:20:10 (24.4 MB/s) - â€˜small_3.txt.2â€™ saved [56513/56513]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'small_3.txt'\n"
      ],
      "metadata": {
        "id": "wpwb0BeyDWo8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def load_dataset(path,tokenizer):\n",
        "    dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return dataset,data_collator\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
        "train_dataset,data_collator = load_dataset(train_path,tokenizer)\n",
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1-dtEE0Ecbp",
        "outputId": "a8ab67af-dcc2-449f-9f24-b772d530064d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 363 MB.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for quantised loading\n",
        "from torch import float32, nn, exp\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x).to(float32)\n",
        "\n",
        "\n",
        "def prepare_model(model):\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False  # freeze the model - train adapters later\n",
        "      if param.ndim == 1:\n",
        "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "        param.data = param.data.to(float32)\n",
        "    model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "    model.enable_input_require_grads()\n",
        "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
        "    return model"
      ],
      "metadata": {
        "id": "THyX3uMdlXvF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "from peft import LoraConfig, PeftModel, PeftConfig, get_peft_model\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "lora_config = {\n",
        "    \"r\": 16,# attention heads\n",
        "    \"lora_alpha\": 32, # alpha scaling\n",
        "    \"lora_dropout\": 0.05,\n",
        "    'bias': \"none\",\n",
        "    \"task_type\": \"CAUSAL_LM\", # set this for CLM or Seq2Seq\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bigscience/bloom-3b\", device_map='auto',load_in_8bit=True)\n",
        "model = prepare_model(model)\n",
        "model = get_peft_model(model, LoraConfig(**lora_config))\n",
        "#print(f\"Model trainable parameters:\\n {print_trainable_parameters(model)}\")\n",
        "\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d6QgbMrG4yD",
        "outputId": "03e16491-2e68-4c85-e06e-d1d49aadbd44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-25lp3hjhr9p6w --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 4087 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bloom-3b-small3-v1\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=50, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    fp16= True,\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    #eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "_9lH0vBIsOIE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "psbZA3YMHF2z",
        "outputId": "dc02d924-3c36-44a5-c383-99b9855ae380"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1200 32:56, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.415800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.741800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1200, training_loss=1.3558273824055989, metrics={'train_runtime': 1980.7285, 'train_samples_per_second': 2.348, 'train_steps_per_second': 0.606, 'total_flos': 8446673092608000.0, 'train_loss': 1.3558273824055989, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Short detail about training -\n",
        "- Step\tTraining Loss\n",
        "- 500\t2.263500\n",
        "- 1000\t0.205800\n",
        "- 1500\t0.029900\n",
        "\n",
        "Took about 6 hours in Colab Free in TPU runtime\n",
        "\n",
        "Stopped at Epoch 35 as the Training loss was pretty low\n",
        "\n",
        "1655/2350 7:15:15 < 3:03:00, 0.06 it/s, Epoch 35.19/50]"
      ],
      "metadata": {
        "id": "kavOmpTpVHy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "aVehIp-JHwsO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.to_json_file(\"config.json\")"
      ],
      "metadata": {
        "id": "YzEtJVE41kSz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r bloom-3b-small3-v1.zip bloom-3b-small3-v1/config.json  bloom-3b-small3-v1/training_args.bin  bloom-3b-small3-v1/pytorch_model.bin bloom-3b-small3-v1/generation_config.json\n"
      ],
      "metadata": {
        "id": "PttMdSZY_9_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'bloom-3b-small3-v1.zip')"
      ],
      "metadata": {
        "id": "A4Xec82EAmgr"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp bloom-3b-small3-v1.zip ./drive/MyDrive/models"
      ],
      "metadata": {
        "id": "wN2AZY_xCPZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "XaH_ZIfUmGAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uECqpfoO5yQd",
        "outputId": "a60fd2cf-4ec9-4e48-fe0f-d5b3e1f2095c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): BloomForCausalLM(\n",
              "      (transformer): BloomModel(\n",
              "        (word_embeddings): Embedding(250880, 2560)\n",
              "        (word_embeddings_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (h): ModuleList(\n",
              "          (0-29): 30 x BloomBlock(\n",
              "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (self_attention): BloomAttention(\n",
              "              (query_key_value): Linear8bitLt(\n",
              "                in_features=2560, out_features=7680, bias=True\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=7680, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): BloomMLP(\n",
              "              (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
              "              (gelu_impl): BloomGelu()\n",
              "              (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): CastOutputToFloat(\n",
              "        (0): Linear(in_features=2560, out_features=250880, bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp ./drive/MyDrive/models/bloom-3b-small3-v1.zip . #if you are taking the fine tuned model from drive"
      ],
      "metadata": {
        "id": "Kivg7RGU78fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip bloom-3b-small3-v1.zip"
      ],
      "metadata": {
        "id": "Uqtm8zCf724M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9baa91fe-216d-4576-dc34-31d092104224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bloom-560-small3-v1.zip\n",
            "  inflating: bloom-560-small3-v1/config.json  \n",
            "  inflating: bloom-560-small3-v1/training_args.bin  \n",
            "  inflating: bloom-560-small3-v1/pytorch_model.bin  \n",
            "  inflating: bloom-560-small3-v1/generation_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#test = pipeline('text-generation',model='./bloom-3b-small3-v1/', tokenizer='bigscience/bloom-3b')\n",
        "test = pipeline('text-generation',model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "XN-eoDuiHXPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6742cc25-6bc7-4b42-ed95-25f5bcd8ee98"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  prompt = \"what is bacteria\"\n",
        "  encoded_input = tokenizer(prompt,truncation=True,padding=True, return_tensors='pt')\n",
        "  test_output_2 = model.generate(input_ids=encoded_input.input_ids,\n",
        "                  max_new_tokens=100,\n",
        "                  num_return_sequences=1,\n",
        "                  early_stopping=True)\n",
        "  test_answer_2 = tokenizer.decode(test_output_2[0], skip_special_tokens=True)\n",
        "  print(f\"Generated test_answer_1 : {test_answer_2}\")\n"
      ],
      "metadata": {
        "id": "CKyPR4inIg9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190a654c-229e-4c79-ee88-1c86420fb93a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated test_answer_1 : what is bacteria called æŠµæŠ— åŠ›). æŠµæŠ—åŠ›ç¬¬ä¸€æ¬¡è¢«åˆ©ç”¨æ¥æ²»ç—…ï¼Œæ˜¯ç”±æ³•å›½åŒ»ç”ŸèŽ«å¥ˆåœ¨1805å¹´é¦–å…ˆé‡‡ç”¨ã€‚ä»–æ³¨å°„ä¼¤å¯’æ†èŒæˆ–é¼ ä¼¤å¯’æ†èŒï¼Œä½¿æ‚£è€…äº§ç”ŸæŠµæŠ—åŠ›ï¼Œä»¥è‡´åŽæ¥ä¸å¿…ç”¨éš”ç¦»æ³•ï¼Œå³å¯æ²»æ„ˆä¼¤å¯’ç—…ã€‚è¿™ç§æ³¨å°„ç–—æ³•ï¼Œè‡³ä»Šä»åœ¨ç”¨ã€‚æ­¤å¤–ï¼Œçƒ§ç¼ç–—æ³•ã€å–·é›¾ç–—æ³•ã€å†²æ´—ç–—æ³•ã€æ•·è´´ç–—æ³•ã€å¡žå…¥ç–—æ³•ã€ä½©å¸¦å£ç½©ç–—æ³•ã€ä½©å¸¦æ‰‹å¥—ç–—æ³•ã€ä½©å¸¦æŠ¤ç›®\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the passage\n",
        "[ An alkaline medium favours bacterial growth; and moisture is a necessary condition; spores, however, can survive the want of water for much longer periods than fully developed bacteria. The necessity for oxygen varies in different species. Those that require oxygen are known as  aÃ«robic bacilli  or  aÃ«robes ; those that cannot live in the presence of oxygen are spoken of as  anaÃ«robes . The great majority of bacteria, however, while they prefer to have oxygen, are able to live without it, and are called  facultative anaÃ«robes"
      ],
      "metadata": {
        "id": "OAFPCf2kUgEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out = test('Streptococci are met with in', max_new_tokens=120,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5fhnJ8XFQ3Y",
        "outputId": "c1913f41-28d3-4f20-ff88-20c63aec2a48"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Streptococci are met with in greater abundance than those of the pneumococcus, the staphylococcus being often the only species present. Streptococci are not so frequently met with in the blood as the pneumococcus, and it is not uncommon to find them in the blood even after the patient has been successfully treated with penicillin. Streptococci are not so easily killed by the strong chaleur, or by the antiseptics, as the pneumococcus or the staphylococcus.  Acid-fast Bacilli  are the only micro-organisms with which the path of disease is absolutely to be determined. They'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inital part is exaclty as in the passage [Streptococci are met with in erysipelas and various other inflammatory and suppurative processes of a spreading character.  Bacilli  are rod-shaped bacteria, usually at least twice as long as they are broad (Fig. 4). Some multiply by fission, others by sporulation. Some forms are motile, others are non-motile.]\n",
        "\n",
        "The last line\n",
        "\n",
        "from passage [Tuberculosis, tetanus, anthrax, and many other surgical diseases are due to different forms of bacilli.  Spirilla  are long, slender, thread-like cells, more or less spiral or wavy.]\n",
        "\n",
        "The last line generated is not correct\n",
        "\n",
        "\"Some forms are motile only in virtue of the contractility of the protoplasm, \"\n",
        "\n",
        "is there in another pace where \"motile\" is reffered\n",
        "\n",
        "whereas the following is pure hallicunation\n",
        "\n",
        "\"some in virtue of the fibroblasts which they carry. Others are'}]\""
      ],
      "metadata": {
        "id": "ZD9lCgUOW_qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True): # else RuntimeError: expected scalar type Half but found Float\n",
        "  out =test('Streptococci', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsN0r9eeYUgJ",
        "outputId": "db3aa6e6-34d7-4d37-8ff2-fc4b2dd21f20"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Streptococci, Streptococcus pyogenes, being the most important. It is only necessary to introduce a small amount of culture in the more recent stages of erysipelas, to the exclusion of all power of recovery; and it is not improbable that the patient may be rendered tolerant of the organisms that are obtained.  Hygienic considerations.  It is impossible to ensure aseptic handling of diseased tissue or of vital substances, and it is therefore important that the instruments should not only be'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out =test('Metchnikoff', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdvxhuI2q3Ki",
        "outputId": "cbc16548-22c2-49f7-bfbe-4e68be1d4080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Metchnikoff's disease, diabetes, syphilis, scurvy, or alcoholism, also impedes healing. Infection by disease-producing micro-organisms or  pathogenic bacteria  is, however, the most potent factor in disturbing the natural process of repair in wounds.\\n\\nSURGICAL BACTERIOLOGY The influence of micro-organisms in the causation of disease, and the rÃ´le played by them in interfering with the natural process of repair, are so important that the science of applied\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out =test('To this process Metchnikoff', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQnX1haEq28P",
        "outputId": "48b8ea34-3083-4d0e-cc72-466f2ed94fb2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'To this process Metchnikoff ascribe suturing powers, which he believes to be universal, as well as that the operators should be free from all infection of the disease to which the wounds are subjected. For the treatment of sepsis the precautions required in suturing viva voce organisms should be gone to, are much the same as in suturing pure culture.  First-aid Ø¶Ù…Ø§Ø¯Ù‡  or  bandage Ø¶Ù…Ø§Ø¯Ù‡ is absolutely necessary when the patient is in the acute stages of disease, and when the wounds'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out = test('phagocytosis', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89o6DTDRHnZ4",
        "outputId": "b4da9b70-dee9-4535-f8e2-0edc3baf9059"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'phagocytosis, and in the production of  hydroxypenicillin  from the bacteria. b. By virtue of its being affected by the vital changes in the bacteria, the bacterium is no longer capable of producing its usual products. Thus, when a pyogenic or a toxenic bacillus is affected, such as producing less acid than usual, or when the environment of the cells is altered, such as becoming more moist, the vital properties of the bacteria diminish, and they become susceptible to'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out =test('During the process of phagocytosis,', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ls3NLVH8FK",
        "outputId": "b778add8-62a2-4481-a779-18ffd7cc2cd4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'During the process of phagocytosis, the granulation tissue formed around the edges of the wound acts as a scaffolding to hold the edges together, and in a few days the scaffolding is replaced by granulation tissue, which is characterised by the presence of fibrin in the vessels of the scaffolding and by the presence of leucocytes in the vessels of the new circulation. In the large vessels of the scaffolding the fibrin is coagulated by the heat generated by the bacteria, which are thus killed, and the products of'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out =test(' diplococci ', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWXYJWCtmdIA",
        "outputId": "6bf02f6c-c910-407c-9362-52341202a213"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': ' diplococci   streptococci   pyogÃ¨nes   toxines   bacilli de la tuberculose  etcã€‚  b l l i b l l i s s t e r s  s t e r i n s  i n t e r n e s  i n t e r n e r  i n t e r n e r i n s   Bulle  Bulle liquide  ou  liquide   ou  liquide   ou  sÃ©rum   ou'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out = test('Cocci  or  micrococci', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwWTYGjzoQfF",
        "outputId": "0a44dc44-8418-4402-cc69-32651f1c1bb7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Cocci  or  micrococci.  The production of gas by the bacteria may cause them to appear in the surface as papillae, as  staphylococci, for example, when the bacteria produce gas by fermentation the acid produced oxidises the tissue around it and causes a darker area to appear: these areas are called  cocci, or  acid bacteria, and are usually associated with more general changes in the diseased tissue: see  acid reaction (diathesis) and  cocci reaction (diagnosis'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out =test('Bacteria are most conveniently', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JszU6_YksrTI",
        "outputId": "cbb3bd45-dab7-4a62-c98e-adba6cd11e73"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Bacteria are most conveniently cultivated in media made up of glucose as the main ingredient. They require no supplement other than a moderately acid medium, and the atmosphere must be free from all animal life for a period of from 14 to 21 days. The most common bacteria cultivated are described in the next chapter.  Bacteria  of Malignant Character  are characterised by a high degree of metaplasia of the bacteria. The metaplasia is a change in the nature of the bacteria, the original'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(cache_enabled=True):\n",
        "  out = test('given the context \"Thus we recognise (1) those that are globular  cocci ; (2) those that resemble a rod  bacilli ; (3) the spiral or wavy forms  spirilla .  Cocci  or  micrococci  are minute round bodies, averaging about 1 Âµ in diameter. The great majority are non-motile. They multiply by fission; and when they divide in such a way that the resulting cells remain in pairs, are called  diplococci , of which the bacteria of gonorrhÅ“a and pneumonia are examples (Fig. 5). When they divide irregularly, and form grape-like bunches, they are known as  staphylococci , and to this variety the commonest pyogenic or pus-forming organisms belong' +\n",
        "  'answer \"What are Cocci  or  micrococci', max_new_tokens=100,num_return_sequences=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ujnSq4EZ0_l",
        "outputId": "686aa0e6-411c-48b0-e658-d1ee6c3878dd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'given the context \"Thus we recognise (1) those that are globular  cocci ; (2) those that resemble a rod  bacilli ; (3) the spiral or wavy forms  spirilla .  Cocci  or  micrococci  are minute round bodies, averaging about 1 Âµ in diameter. The great majority are non-motile. They multiply by fission; and when they divide in such a way that the resulting cells remain in pairs, are called  diplococci , of which the bacteria of gonorrhÅ“a and pneumonia are examples (Fig. 5). When they divide irregularly, and form grape-like bunches, they are known as  staphylococci , and to this variety the commonest pyogenic or pus-forming organisms belonganswer \"What are Cocci  or  micrococci?\" and \"Spiral or Wavy Forms?\" below (p. 57).  The diplococci of the skin and of other surfaces are drawn into groups into which the name  cicatricial  is applied, as being used in a reparative process. In the eye the diplococci drawn from the retina are a source of the disease-trouble which they bring about is due to the fact that, when the retina is invaded by the growth, vision is lost for'}]\n"
          ]
        }
      ]
    }
  ]
}